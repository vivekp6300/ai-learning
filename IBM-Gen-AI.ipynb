{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f87b06f-d89f-4019-8c1b-1b01488c5b3f",
   "metadata": {},
   "source": [
    "# IBM RAG and Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb99bd-0b82-4701-b4f8-7e6347495018",
   "metadata": {},
   "source": [
    "## Course 3 - Vector Databases for RAG: An Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a88fdd-06d2-4a5f-b421-e4bad5ab01c2",
   "metadata": {},
   "source": [
    "### Module 1 - Introduction to Vector Databases and Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c60364-091b-41ce-a487-2b52310f5049",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 1 - Vector Database Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec999fe-cd3a-4a5d-836a-640b7e470e40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Vector DBs can be used to group items, classify items and suggest relationships among items\n",
    "- Vector DBs can be used\n",
    "    - to store complex data types (social likes, geospatial data, genomic data etc)\n",
    "      <img src='images/200754.008_Vector-DB-reading-image1.png' width=600/>\n",
    "    - perform similarity searches\n",
    "    - for diverse domains like biology, healthcare, e-commerce, social media and traffic planning)\n",
    "    - to support machine learning\n",
    "- Traditional DBs store data as tables, Vector DBs store data as high dimensional vectors with size and direction. Each dimension relates to different attributes. For e.g. a book can be stored in vector DB as [1, 300, 2024, 4.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6adddb-c3ed-4712-b524-ddd3015354c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 2 - Traditional vs Vector Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa3ee4-90d8-4727-a220-a6dcd5ed719f",
   "metadata": {},
   "source": [
    "|Function|Traditional databases|Vector databases|\n",
    "|------|------|------|\n",
    "|Data Representation|Traditional databases organize data in a structured format using tables, rows, and columns, ideal for relational data|Vector databases represent data as multi-dimensional vectors, efficiently encoding complex and unstructured data like images, text, and sensor data.|\n",
    "|Data Search and Retrieval|SQL queries are suited for traditional databases with structured data.|Vector databases specialize in similarity searches and retrieving vectorized data, facilitating tasks like image retrieval, recommendation systems, and anomaly detection.|\n",
    "|Indexing|Traditional databases employ indexing methods like B-trees for efficient data retrieval.|Vector databases use indexing structures like metric trees and hashing suited for high-dimensional spaces, enhancing nearest-neighbor searches and similarity assessments.|\n",
    "|Scalability|Scaling traditional databases can be challenging, often requiring resource augmentation or data sharding.|Vector databases are designed for scalability, especially in handling large datasets and similarity searches, using distributed architectures for horizontal scaling.|\n",
    "|Applications|Traditional databases are pivotal in business applications and transactional systems where structured data is processed.|Vector databases shine in analyzing vast datasets, supporting fields like scientific research, natural language processing, and multimedia analysis.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef86a4-8d7f-43b9-8ccf-a3be223d7c0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 3 - Vector Database Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc7c0d-55c1-4796-81ba-e6a6b330cc56",
   "metadata": {},
   "source": [
    "- **In-memory** Vector DBs e.g. *RedisAI, Torchserve* store vectors in RAM hence fast but limited in size\n",
    "- **Disk-Based** Vector DBs e.g. *Annoy, Milvus, ScaNN* store vectors on disk, use compression and indexing and are suitable for large datasets\n",
    "- **Distributed** Vector DBs e.g. *FAISS, ElasticSearch+, Dask-ML* spread data across multiple nodes/servers hence great for horizotnal scaling and fault tolerance making them suitable for large datasets with fast retrieval\n",
    "- **Graph Based** Vector DBs e.g. *Neo4J, Amazon Neptune, TigerGraph* model data as a graph with nodes and edges representing attributes. They are great at capturing complex relationships and graph analytics\n",
    "- **Time Series** Vector DBs e.g. *InfluxDB, TimescaleDB, Prometheus* represent data collected over time as vectors and are good for identifying temporal patterns and anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd5208-01b6-4309-a86d-f63bc3402f46",
   "metadata": {},
   "source": [
    "Vector DBs can also be classified as dedicated vector DBs or DBs that support vector search\n",
    "\n",
    "**Dedicated Vector DBs**\n",
    "- use unique data structures like reverse indexes, product quantization and Locality-sensitive Hashing (LSH)\n",
    "- support vector operations like similarity search, nearest neighbour search and distance calculations\n",
    "- provide scaleability through clustering or distributed nodes\n",
    "- deliver speed through optimized algorithms and data structures\n",
    "- are customisable by changing parameters of indexing and searching as per application needs\n",
    "- Examples are FAISS, Annoy and Milvus\n",
    "\n",
    "**Databases that support Vector Search**\n",
    "- are regular DBs or data processing frameworks that have tools and addons to allow users to do vector search and other queries\n",
    "- Store data as part of their data model as BLOBs, Arrays or UDTs.\n",
    "- Allow standard and custom indexing to organise data\n",
    "- Have add-on libraries and plugins to support vector operations\n",
    "- Not as optimized or fast as dedicated vector DBs\n",
    "- Examples are SingleStore (works with watsonx.ai), ElasticSearch, PostgreSQL, MySQL, RedisAI, Apache MongoDB and Apache Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa0423-de48-431a-ba81-78746839c074",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 4 - Applications of Vector DBs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc13dd-75c6-42c6-b68b-f14ba5e37eee",
   "metadata": {},
   "source": [
    "1. **Image and Video Analysis**\n",
    "|Task|Capability|Uses|\n",
    "|------|------|------|\n",
    "|Feature Extraction & Representation|Store High-Dimensional Feature Vectors|Displays aspects of images, such as color histograms, texture descriptions or deep learning embeddings|\n",
    "|Similarity Searches|Store Feature Vectors|Locate images, Summarize videos, and suggest images and videos based on content|\n",
    "|Process Real-time data|Provide horizontal scaling for real-time storage|Perform video surveillance, object recognition, and live event analysis|\n",
    "\n",
    "2. **Recommendation Systems**\n",
    "|Task|Capability|Uses|\n",
    "|------|------|------|\n",
    "|Embedding Storage and Nearest Neighbour Search|Incorporate embeddings or numerical representations of items or entities generated by a recommendation system|Access the vector's likes and traits, Locate the vector's closest neighbours for improved personalized suggestions|\n",
    "|Deliver performance improvement and scalability|Provide scalability to handle additional searches and vectors. Improve query processing and indexing structure|Deliver fast, scalabale recommendation services for large numbers of concurrent users|\n",
    "|Provide cross-domain suggestions|Store embeddings and carry out cross-domain suggestions|Enhance the completeness of recommendation systems|\n",
    "\n",
    "3. **Geospatial analysis and location-based services**\n",
    "|Task|Capability|Uses|\n",
    "|------|------|------|\n",
    "|Efficiently store and index data| * Use indexing methods like R-tree or quad tree * Store geospatial data like addresses, polygons, GPS locations | Deliver spatial queries like closeness searches, range queries and spatial joins, for GPS information and other mapping needs|\n",
    "|Provide location-based suggestions|Combine geospatial data with user preferences and location|Deliver recommendations for nearby events, services and places of interest|\n",
    "|Deliver realtime geospatial analytics|*Process streaming data in real-time * Groups items together spatially * Recognizes spatial patterns|Power apps like tracking vehicles, managing fleets, dynamic routing, finding hotspots|\n",
    "\n",
    "4. **Marketing and social media insights**\n",
    "\n",
    "|Task|Capability|Uses|\n",
    "|------|------|------|\n",
    "|Provide distributed storage and parallel processing for horizontal scalability|Spread data and queries across multiple nodes or groups|Process big data and handle simultaneous queries such as SEO calculations|\n",
    "|Reduce latency and boost overall speed|Use optimized caching and query execution plans|Obtain trending analytics faster|\n",
    "|Adjust to changing task needs|Support auto-scaling and dynamic resource allocation|Your company can scale hardware and cloud resource usage for the best performance and lower costs|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30bf82-3af2-45ce-af98-9477cc8b11d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 5 - Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553731d7-7b0a-4dbb-b2b4-fc6acd3bdf38",
   "metadata": {},
   "source": [
    "For any two vectors $\\vec{a}$ and $\\vec{b}$ \n",
    "- the **L2 distance** or Eucliendian distance $\\sqrt{\\sum{(a_i-b_i)}^2}$ is a **distance** metric\n",
    "- the **dot product** $\\sum{a_i b_i}$ or $\\lVert{a}\\rVert \\lVert{b}\\rVert cos(\\alpha)$ is a **similarity** metric. However its negative can be used as a distance metric (larger dot product $\\implies$ less distance)\n",
    "- The **cosine similarity** cosine_similarity(a,b) $=\\frac{a . b}{\\lvert{a}\\rVert \\lvert{b}\\rVert} = \\frac{a}{\\lvert{a}\\rVert} \\frac{b}{\\lvert{b}\\rVert} = norm(a) \\times norm(b)$ is a **similarity** metric and (1-cosine_similarity) is a **distance** metric\n",
    "\n",
    "|Metric|Sensitive to Magnitude|Normalised|Best For|\n",
    "|------|------|------|------|\n",
    "|L2 Distance|$\\checkmark$Yes|$\\times$No|Spatial Data, Clustering|\n",
    "|Cosine Distance|$\\times$No|$\\checkmark$Yes|Text, Embeddings, NLP|\n",
    "|Dot Product|$\\checkmark$Yes|$\\times$No|Neural Networks, recommender systems|\n",
    "\n",
    "L2 distance works well for continuous, lower-dimensional data where magnitude matters. \n",
    "\n",
    "Cosine distance excels with high-dimensional, sparse data where direction is more important than magnitude. \n",
    "\n",
    "Dot product offers computational efficiency and is useful when both magnitude and direction contribute to similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25bc29-a8df-40a3-ba4f-6c57e486928a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Lab on manually implementing Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca443f-c385-4742-8248-d7d8a9800b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers==4.1.0 | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407f0ac-576b-40ab-87b7-c9c166881593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077a691-6a72-4979-be62-a12a475973e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "documents = [\n",
    "    'Bugs introduced by the intern had to be squashed by the lead developer.',\n",
    "    'Bugs found by the quality assurance engineer were difficult to debug.',\n",
    "    'Bugs are common throughout the warm summer months, according to the entomologist.',\n",
    "    'Bugs, in particular spiders, are extensively studied by arachnologists.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2124a22-b3cf-4b10-af47-b3aa20d5420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d621be6-07a2-4eb6-8cb9-c108aa646bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561d9e5-615a-460a-9d03-48b193ba85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86f2a0-6cbd-4574-a1d4-32a67bd97de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fa3a1-4918-4aff-91c2-564817bf3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_fn(vector1, vector2):\n",
    "    squared_sum = sum((x - y) ** 2 for x, y in zip(vector1, vector2))\n",
    "    return math.sqrt(squared_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede6d57-1b39-465f-9327-fabee654b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance_fn(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdbc54-4c8b-4195-9e86-eb2e09d49dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance_fn(embeddings[1], embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731e0e7-b754-426f-8a12-2e1819f4805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist_manual = np.zeros([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        l2_dist_manual[i,j] = euclidean_distance_fn(embeddings[i], embeddings[j])\n",
    "\n",
    "l2_dist_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496962c-529c-47c4-a4cc-e6b95d32f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist_manual[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa32ea-50ef-414c-b88c-1f672fbe01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist_manual[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5b2e9-eb8d-4189-83d3-72aabfa9aa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist_manual_improved = np.zeros([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        if (i>j):\n",
    "            l2_dist_manual_improved[i,j] = l2_dist_manual_improved[j,i]\n",
    "        elif (i<j):\n",
    "            l2_dist_manual_improved[i,j] = euclidean_distance_fn(embeddings[i], embeddings[j])\n",
    "l2_dist_manual_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f2ee1-d137-49ae-a4c9-13326c512bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist_scipy = scipy.spatial.distance.cdist(embeddings, embeddings, 'euclidean')\n",
    "l2_dist_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b5437-704e-4300-9eaf-9087fdf40535",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(l2_dist_manual, l2_dist_scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6929c27-a10c-4dc2-9acc-c9d43934fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_fn(vector1, vector2):\n",
    "    return sum(x * y for x, y in zip(vector1, vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c252f7-d2c1-4591-98c3-3ad7b13f6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_fn(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39caefa-2357-492c-85ae-90e293fa66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_manual = np.empty([4,4])\n",
    "for i in range(embeddings.shape[0]):\n",
    "    for j in range(embeddings.shape[0]):\n",
    "        dot_product_manual[i,j] = dot_product_fn(embeddings[i], embeddings[j])\n",
    "\n",
    "dot_product_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f9a28-29fd-493b-ba19-462c9200f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication operator\n",
    "dot_product_operator = embeddings @ embeddings.T\n",
    "dot_product_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519e13a-5062-4fb2-9979-5549b377b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(dot_product_manual, dot_product_operator, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18e273-8d84-4a52-bac2-60bd720d4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to `np.matmul()` if both arrays are 2-D:\n",
    "np.matmul(embeddings,embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa84357-5a4f-4830-8485-4c263ae6ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `np.dot` returns an identical result, but `np.matmul` is recommended if both arrays are 2-D:\n",
    "np.dot(embeddings,embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046db09-ee65-4f21-be0d-47299453f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_distance = -dot_product_manual\n",
    "dot_product_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50590bba-f1ad-4fb5-851c-f30f7f9e2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 norms\n",
    "l2_norms = np.sqrt(np.sum(embeddings**2, axis=1))\n",
    "l2_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede00f3-8140-45ba-b027-1f379a3f116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 norms reshaped\n",
    "l2_norms_reshaped = l2_norms.reshape(-1,1)\n",
    "l2_norms_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec045f-d251-45e1-af1b-bec6cf9c5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_embeddings_manual = embeddings/l2_norms_reshaped\n",
    "normalized_embeddings_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6c69b-b186-4ef7-8e9b-9f780723a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.sqrt(np.sum(normalized_embeddings_manual**2, axis=1)),np.array([1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a252dcb-b273-44ec-9190-9b1e17e48c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_embeddings_torch = torch.nn.functional.normalize(\n",
    "    torch.from_numpy(embeddings)\n",
    ").numpy()\n",
    "normalized_embeddings_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955af0ec-d8b1-48ad-b7b5-64637791b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(normalized_embeddings_manual, normalized_embeddings_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78433ac-adca-4f0c-8665-261edc5c7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_fn(normalized_embeddings_manual[0], normalized_embeddings_manual[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d3dba-ae77-414f-9fe2-318e6ebd215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_manual = np.empty([4,4])\n",
    "for i in range(normalized_embeddings_manual.shape[0]):\n",
    "    for j in range(normalized_embeddings_manual.shape[0]):\n",
    "        cosine_similarity_manual[i,j] = dot_product_fn(\n",
    "            normalized_embeddings_manual[i], \n",
    "            normalized_embeddings_manual[j]\n",
    "        )\n",
    "\n",
    "cosine_similarity_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614997d2-b370-4533-993c-b227d06e81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_operator = normalized_embeddings_manual @ normalized_embeddings_manual.T\n",
    "cosine_similarity_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08151aa0-4816-4bda-9df9-e116a6df5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(cosine_similarity_manual, cosine_similarity_operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33673e54-9c7c-40dd-a62d-1ce9e932ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - cosine_similarity_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5472d61-d9a8-435f-b592-6fa42f87e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE GOES HERE ###\n",
    "# First, embed the query:\n",
    "query_embedding = model.encode(\n",
    "    [\"Who is responsible for a coding project and fixing others' mistakes?\"]\n",
    ")\n",
    "\n",
    "# Second, normalize the query embedding:\n",
    "normalized_query_embedding = torch.nn.functional.normalize(\n",
    "    torch.from_numpy(query_embedding)\n",
    ").numpy()\n",
    "\n",
    "# Third, calculate the cosine similarity between the documents and the query by using the dot product:\n",
    "cosine_similarity_q3 = normalized_embeddings_manual @ normalized_query_embedding.T\n",
    "\n",
    "# Fourth, find the position of the vector with the highest cosine similarity:\n",
    "highest_cossim_position = cosine_similarity_q3.argmax()\n",
    "\n",
    "# Fifth, find the document in that position in the `documents` array:\n",
    "documents[highest_cossim_position]\n",
    "\n",
    "# As you can see, the query retrieved the document `Bugs introduced by the intern had to be squashed by the lead developer.` which is what we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f730e908-6ad1-4c21-8fba-1d2d7948a89b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 6 - Chroma DB Key Concepts and Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d7cf6-7b4f-4498-9c13-d0adaf2c5df3",
   "metadata": {},
   "source": [
    "**Chroma DB Capabilities**\n",
    "- Storage of embeddings and their metadata\n",
    "- Vector Search\n",
    "- Full-text Search\n",
    "- Document Storage\n",
    "- Metadata Filtering\n",
    "- Multi-Modal Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d8a69-7526-409c-bbc2-827b30a0277d",
   "metadata": {},
   "source": [
    "**Deployment Modes**\n",
    "- *Client Server Architecture*: Client and Server run as independent processes, server is launched through CLI or Docker Image and client connects to server over HTTP\n",
    "- *Standalong Mode*: Meant for Python only, client and server run in same process, useful for capabilities demo or when it is clear that only one machine would be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8ce73-c464-48ce-ae9c-a68d317d5e86",
   "metadata": {},
   "source": [
    "**Architecture Phases**\n",
    "1. Obtaining Embeddings (Optional, Chroma DB can do this automatically)\n",
    "2. Creating Collections\n",
    "3. Storing Data (need to pass embeddings if Chroma DB is not handling embedding internally)\n",
    "4. Performing Collection Operations (Update/Delete/Rename etc)\n",
    "5. Querying and Grouping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe82edd-7b59-4929-909c-33c87ad32894",
   "metadata": {},
   "source": [
    "**Ecosystem - Clients and Integrations**\n",
    "- Officially supports Python and JS clients. Community supports Java, Ruby, C#, Go, Rust, PHP etc\n",
    "- Integrates with Langchain, LlamaIndex and OLlama\n",
    "- Provides native integrations with HuggingFace, Google and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cd0f9-2548-4260-b3f4-2834bc2e25a3",
   "metadata": {},
   "source": [
    "**In Practice**\n",
    "Steps to execute\n",
    "1. Create Collection\n",
    "2. Add Text Chunks + Metadata (ChromaDB handles embeddings, else you pass embeddings)\n",
    "3. Query Collection (most similar results returned, query embedding handled internally. Similarity by default is Euclidean (L2) Distance. Dot Product and Cosine Similarity are also supported.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58183bf2-357d-40e3-a0d3-885625ff1ee2",
   "metadata": {},
   "source": [
    "**Performance Features**\n",
    "- Efficient Similarity Search\n",
    "    - Optimized for Nearest Neighbour Search\n",
    "    - Internally uses HNSW Algorithm (Hierarchical Navigable Small World)\n",
    "- Coding Practices\n",
    "    - Written in Rust $\\implies$ 3-5 times improvement in querying and writing ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb2f99-5f7b-4909-b4f6-49481bc96716",
   "metadata": {},
   "source": [
    "**Use Cases and Applications**\n",
    "- Recommender Systems\n",
    "- Document Search with vector or full text search\n",
    "- Image Retrieval, based on text queries using multi-modal retrieval\n",
    "- AI-based chatbots built with semantic search and retrieval capabilities for context augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99e674-f043-4226-a5e5-85fe3cf97a03",
   "metadata": {},
   "source": [
    "**Filtering**\n",
    "- Supports **Metadata filtering** (similar to SQL where clauses but more powerful) and **Document Filtering** (similar to SQL contains clauses but more powerful)\n",
    "- **Metadata Filtering**\n",
    "    - Syntax `collection.get(where={\"key\":\"value\"})` (works for `.delete()`and `.query()`also)\n",
    "    - Following operators are also supported in metadata filtering: `$eq, $ne, $gt, $lt, $gte, $lte, $in, $nin` as `where={\"key\":{\"$eq\":\"value\"}}, where={\"key\":{\"$in\":[\"value1\", \"value2\"]}}` etc\n",
    "\n",
    "    - Filters can be combined as `and`or `or` using the syntax below\n",
    "    ```python\n",
    "    collection.get(\n",
    "        where={\"$and\":[\n",
    "            {\"key1\":{\"$gte\":\"value1\"}}, \n",
    "            {\"key2\":{\"$lt\":\"value2\"}}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "- **Document Filtering**\n",
    "    - Syntax `collection.get(where_document={\"$contains\":\"value\"})`. `not_contains`is also supported similarly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4bba4d-4e65-4570-a623-ae253564e408",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Working Example of ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e447152-ded0-4f08-837a-2306cdc5e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5723c69-2bc1-40fd-ba25-80b11a1bd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "client=chromadb.Client()\n",
    "\n",
    "collection_name = \"filter_demo\"\n",
    "\n",
    "try:\n",
    "    client.delete_collection(collection_name)\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "collection=client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"description\":\"Used to demo filtering in ChromaDB\"},\n",
    "    configuration={\n",
    "        \"embedding_function\":ef\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Collection Created: {collection.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cc02f-f180-4218-be14-489d17652492",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\n",
    "        \"This is a document about LangChain\",\n",
    "        \"This is a reading about LlamaIndex\",\n",
    "        \"This is a book about Python\",\n",
    "        \"This is a document about pandas\",\n",
    "        \"This is another document about LangChain\"\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"source\": \"langchain.com\", \"version\": 0.1},\n",
    "        {\"source\": \"llamaindex.ai\", \"version\": 0.2},\n",
    "        {\"source\": \"python.org\", \"version\": 0.3},\n",
    "        {\"source\": \"pandas.pydata.org\", \"version\": 0.4},\n",
    "        {\"source\": \"langchain.com\", \"version\": 0.5},\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\", \"id3\", \"id4\", \"id5\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766c345-54d2-4400-bc0b-f52eac2065f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all documents where the source is \"langchain.com\"\n",
    "collection.get(\n",
    "    where={\"source\": {\"$eq\": \"langchain.com\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f309c-bf5a-4520-85fc-c49376938a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all documents where the source is \"langchain.com\" with versions less than 0.3\n",
    "collection.get(\n",
    "    where={\n",
    "        \"$and\": [\n",
    "            {\"source\": {\"$eq\": \"langchain.com\"}}, \n",
    "            {\"version\": {\"$lt\": 0.3}}\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4545df-3a97-424f-a657-dbd044be7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves all documents about LangChain and LlamaIndex with a version less than 0.3\n",
    "collection.get(\n",
    "    where={\n",
    "        \"$and\": [\n",
    "            {\"source\": {\"$in\": [\"langchain.com\", \"llamaindex.ai\"]}}, \n",
    "            {\"version\": {\"$lt\": 0.3}}\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddf483-42cb-4db9-991f-f1bfd0cc0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs a full text search for such documents\n",
    "collection.get(\n",
    "    where_document={\"$contains\":\"pandas\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebd744-5351-419c-8904-a694ac40fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks for all documents containing \"LangChain\" or \"Python\" with version numbers greater than 0.1\n",
    "collection.get(\n",
    "    where={\"version\": {\"$gt\": 0.1}},\n",
    "    where_document={\n",
    "        \"$or\": [\n",
    "            {\"$contains\": \"LangChain\"},\n",
    "            {\"$contains\": \"Python\"}\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ba716-7401-4639-a0a1-4d0a38981c88",
   "metadata": {},
   "source": [
    "##### **Similarity Search and HNSW in Chroma DB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f01b7-d454-4413-8fd8-f73b1af428e4",
   "metadata": {},
   "source": [
    "**Vector Indexes** are specialized data structures that enable algorithms to compute similarity scores with only a small subset of vectors, significantly speeding up the search while still returning exact or near-optimal results.\n",
    "\n",
    "One such vector index is **Hierarchical Navigable Small World or HNSW**\n",
    "\n",
    "HNSW builds a multi-layered graph where:\n",
    "\n",
    "- The upper layers contain a sparse overview of the data for fast navigation.\n",
    "- The bottom layer holds all vectors for detailed search.\n",
    "\n",
    "Each vector connects to a few nearby neighbors, forming a \"small world\" network—meaning most vectors can be reached in just a few steps.\n",
    "\n",
    "HNSW is fast, acurate, scaleable, and versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd7278-faa5-4596-9dd6-d1bf5f60e4ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **HNSW setup in ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422b125-21c1-404d-afdd-d74661b4ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Collection creation\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection_name = \"hnsw_demo\"\n",
    "\n",
    "try:\n",
    "    client.delete_collection(name=collection_name)\n",
    "except ValueError:\n",
    "    pass\n",
    "    \n",
    "collection = client.create_collection(\n",
    "    name=collection_name,\n",
    "    metadata={\"topic\": \"query testing\"},\n",
    "    configuration={\n",
    "        \"hnsw\": {\n",
    "            # space can be \"l2\" for L2/Euclidean, \"ip\" for inner dot product or \"cosine\" for cosine similarity\n",
    "            \"space\": \"cosine\",\n",
    "            # ef_search determines size of candidate list when nearest neighbour search is done. Higher values\n",
    "            # increase accuracy but reduce speed\n",
    "            \"ef_search\": 100,\n",
    "            # ef_construction determines size of candidate list used to select nearest neighbours when a new \n",
    "            # node is inserted into the index. Again higher values improve accuracy and reduce speed\n",
    "            \"ef_construction\": 100,\n",
    "            # max_neighbors determines maximum connections a node can have during construction. Higher values \n",
    "            # increase accuracy but also increase cost in terms of memory usage and time. Default value is 16\n",
    "            \"max_neighbors\": 16\n",
    "        },\n",
    "        \"embedding_function\": ef\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8275dc-ce47-44ee-b34b-a2294e01d166",
   "metadata": {},
   "source": [
    "Thus `ef_search` affects the breadth of search, while `ef_construction`and `max_neighbours`affect the quality of the vector index built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d8027-4bb3-4a79-af07-2a926f866260",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **Querying in Chroma DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fbf66-d545-464f-bfe7-0388a74d2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[\n",
    "        \"Giant pandas are a bear species that lives in mountainous areas.\",\n",
    "        \"A pandas DataFrame stores two-dimensional, tabular data\",\n",
    "        \"I think everyone agrees that pandas are some of the cutest animals on the planet\",\n",
    "        \"A direct comparison between pandas and polars indicates that polars is a more efficient library than pandas.\",\n",
    "    ],\n",
    "    metadatas=[\n",
    "        {\"topic\": \"animals\"},\n",
    "        {\"topic\": \"data analysis\"},\n",
    "        {\"topic\": \"animals\"},\n",
    "        {\"topic\": \"data analysis\"},\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\", \"id3\", \"id4\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ae692-b8f7-44f4-a34c-48452fa8556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will result in all 4 documents to be returned, ordered by increasing distance. \n",
    "collection.query(\n",
    "    query_texts=[\"cat\"],\n",
    "    n_results=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2c11a-2717-4002-8071-35f09d83f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will falsely fetch the document #4 confusing the polars library with polar bears. \n",
    "collection.query(\n",
    "    query_texts=[\"polar bear\"],\n",
    "    n_results=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11530b7-fe16-4390-84fa-210b3ca8192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be fixed by using filters along with the query as below\n",
    "collection.query(\n",
    "    query_texts=[\"polar bear\"],\n",
    "    n_results=1,\n",
    "    where={'topic': 'animals'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c7d23-c2a1-4842-b8ad-77f3c8db3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as alternative to metadata filtering, we can also use full document text search filter as below\n",
    "collection.query(\n",
    "    query_texts=[\"polar bear\"],\n",
    "    n_results=1,\n",
    "    where_document={'$not_contains': 'library'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a20c5-faaf-4039-a7a8-7af69bb022a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both metadata filtering and full text search can be combined as well, as below\n",
    "collection.query(\n",
    "    query_texts=[\"polar bear\"],\n",
    "    n_results=1,\n",
    "    where={'topic': 'animals'},\n",
    "    where_document={'$not_contains': 'library'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a01c9-5e4e-4c09-bd28-06d06d0f5562",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lab: Similarity Search on Text Using a Chroma Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e05c6-92dc-4aa4-b543-0bb644fa999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb==1.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda91507-e7fa-4033-ace9-1fc62bfca50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers==4.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a10f3c-faf8-4b5a-89b0-2d677551c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "# Define the embedding function using SentenceTransformers\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed718a8b-d664-4228-8adb-eabf4c2c4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of ChromaClient to interact with the Chroma DB\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Define the name for the collection to be created or retrieved\n",
    "collection_name = \"my_grocery_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38180d-2aab-4199-b7a6-056a43d8bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function to interact with the Chroma DB\n",
    "def main():\n",
    "    try:\n",
    "        # Create a collection in the Chroma database with a specified name, \n",
    "        # distance metric, and embedding function. In this case, we are using \n",
    "        # cosine distance\n",
    "        try:\n",
    "            client.delete_collection(collection_name)\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"A collection for storing grocery data\"},\n",
    "            configuration={\n",
    "                \"hnsw\": {\"space\": \"cosine\"},\n",
    "                \"embedding_function\": ef\n",
    "            }\n",
    "        )\n",
    "        print(f\"Collection created: {collection.name}\")\n",
    "\n",
    "        # Array of grocery-related text items\n",
    "        texts = [\n",
    "            'fresh red apples',\n",
    "            'organic bananas',\n",
    "            'ripe mangoes',\n",
    "            'whole wheat bread',\n",
    "            'farm-fresh eggs',\n",
    "            'natural yogurt',\n",
    "            'frozen vegetables',\n",
    "            'grass-fed beef',\n",
    "            'free-range chicken',\n",
    "            'fresh salmon fillet',\n",
    "            'aromatic coffee beans',\n",
    "            'pure honey',\n",
    "            'golden apple',\n",
    "            'red fruit'\n",
    "        ]\n",
    "        \n",
    "        # Create a list of unique IDs for each text item in the 'texts' array\n",
    "         # Each ID follows the format 'food_<index>', where <index> starts from 1\n",
    "        ids = [f\"food_{index + 1}\" for index, _ in enumerate(texts)]\n",
    "\n",
    "        # Add documents and their corresponding IDs to the collection\n",
    "        # The `add` method inserts the data into the collection\n",
    "        # The documents are the actual text items, and the IDs are unique identifiers\n",
    "        # ChromaDB will automatically generate embeddings using the configured embedding function\n",
    "        collection.add(\n",
    "            documents=texts,\n",
    "            metadatas=[{\"source\": \"grocery_store\", \"category\": \"food\"} for _ in texts],\n",
    "            ids=ids\n",
    "        )\n",
    "\n",
    "        # Retrieve all the items (documents) stored in the collection\n",
    "        # The `get` method fetches all data from the collection\n",
    "        all_items = collection.get()\n",
    "        # Log the retrieved items to the console for inspection\n",
    "        # This will print out all the documents, IDs, and metadata stored in the collection\n",
    "        print(\"Collection contents:\")\n",
    "        print(f\"Number of documents: {len(all_items['documents'])}\")\n",
    "\n",
    "        # Define the query term you want to search for in the collection\n",
    "        # query_term = \"apple\"\n",
    "        \n",
    "        # practice exercise changes query term to a list\n",
    "        query_term = [\"red\",\"fresh\"]\n",
    "\n",
    "        if (isinstance(query_term, str)):\n",
    "            query_term = [query_term]\n",
    "\n",
    "        # Perform a query to search for the most similar documents to the 'query_term'\n",
    "        results = collection.query(\n",
    "            query_texts=query_term,\n",
    "            n_results=3  # Retrieve top 3 results\n",
    "        )\n",
    "        print(f\"Query results for '{query_term}':\")\n",
    "        print(results)\n",
    "\n",
    "        # Check if no results are returned or if the results array is empty\n",
    "        if not results or not results['ids'] or len(results['ids'][0]) == 0:\n",
    "            # Log a message indicating that no similar documents were found for the query term\n",
    "            print(f'No documents found similar to \"{query_term}\"')\n",
    "            return\n",
    "\n",
    "        for q in range(len(query_term)):\n",
    "            print(f'Top 3 similar documents to \"{query_term[q]}\":')\n",
    "            # Access the nested arrays in 'results[\"ids\"]' and 'results[\"distances\"]'\n",
    "            for i in range(min(3, len(results['ids'][q]))):\n",
    "                doc_id = results['ids'][q][i]  # Get ID from 'ids' array\n",
    "                score = results['distances'][q][i]  # Get score from 'distances' array\n",
    "                # Retrieve text data from the results\n",
    "                text = results['documents'][q][i]\n",
    "                if not text:\n",
    "                    print(f' - ID: {doc_id}, Text: \"Text not available\", Score: {score:.4f}')\n",
    "                else:\n",
    "                    print(f' - ID: {doc_id}, Text: \"{text}\", Score: {score:.4f}')\n",
    "\n",
    "        perform_similarity_search(collection, all_items)\n",
    "        \n",
    "        pass\n",
    "    except Exception as error:  # Catch any errors and log them to the console\n",
    "        print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50140f-38ee-4d7c-9d4b-ff45b5a1e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a similarity search in the collection\n",
    "def perform_similarity_search(collection, all_items):\n",
    "    try:\n",
    "        # Place your similarity search code inside this block\n",
    "        pass\n",
    "    except Exception as error:\n",
    "        print(f\"Error in similarity search: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4f8d3-b694-4a63-9fad-3d1ed2dfa130",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87d4ef-a7b5-4358-a5d3-816d578567d3",
   "metadata": {},
   "source": [
    "### Module 2 - Vector Databases for Recommendation Systems and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbf5b1-4003-47e8-979d-4e3f86a13b39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 1 - Essential Database Operations in ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45391d4a-879e-436f-8bf1-ff0cc66f5926",
   "metadata": {},
   "source": [
    "Collections can be\n",
    "- Created: `client.create_collection(name=...)`\n",
    "- Retrieved: `client.get_collection(name=...)`\n",
    "- Modified: `collection.modify(name=..., metadata=...)`. *Not everything can be modified*; embedding model and distance metric cannot be modified - they can be changed only by cloning a new collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b653bc-a4bd-49dc-b3d8-db251d97626b",
   "metadata": {},
   "source": [
    "Documents can be added to a collection by calling `collection.add(...)`\n",
    "\n",
    "```python\n",
    "    collection.add(\n",
    "        documents=[\n",
    "            \"This is document 1\",\n",
    "            \"This is document 2\",\n",
    "            ...], \n",
    "        metadatas=[\n",
    "            {\"topic\":\"X\", \"version\":\"0.1\"},\n",
    "            {\"topic\":\"Y\", \"version\":\"1.2\"}\n",
    "            ...], \n",
    "        ids=[\"id1\",\"ïd2\"...]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f4e7a-e851-426e-9805-89b1c6fd68f3",
   "metadata": {},
   "source": [
    "Documents can be retrieved using `collection.get()` which returns all documents\n",
    "The output looks like below\n",
    "```python\n",
    "{\n",
    "    \"ids\":[\"id1\",\"id2\"],\n",
    "    \"embeddings\":None,\n",
    "    \"documents\":[\"...\",\"...\"],\n",
    "    \"uris\":None,\n",
    "    \"ïncluded\":[\"metadatas\",\"documents\"],\n",
    "    \"data\":None,\n",
    "    \"metadatas\":[{\"\":\"\", \"\":\"\"},{\"\":\"\", \"\":\"\"}]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa085d5-5a77-4af9-a527-0f49a1b4a305",
   "metadata": {},
   "source": [
    "embeddings by default are not returned, but can be obtained by calling\n",
    "```python\n",
    "collection.get(include=[\"embeddings\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30531583-98b2-4ba3-8cd2-5e6dbbbc2364",
   "metadata": {},
   "source": [
    "To retrieve specific document, use `collection.get(\"id1\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e639c-a928-4ac8-9c8a-ea90ee7f556d",
   "metadata": {},
   "source": [
    "To update existing data in collection, use \n",
    "```python\n",
    "collection.update(\n",
    "    ids=[\"id1\"], \n",
    "    metadatas=[{\"topic\":\"New Topic\", \"version\":\"2.0\"}],\n",
    "    documents=[\"Updated document\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a5e75-c6e3-4514-aa40-234ba75cbab3",
   "metadata": {},
   "source": [
    "ChromaDB handles embeddings in the background automatically when document changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b98cd-f055-44bd-b67f-7d6bc0d3303b",
   "metadata": {},
   "source": [
    "Delete documents from collection using either ids, or where clause, or both.\n",
    "```python\n",
    "collection.delete(\n",
    "    ids=[\"id1\",\"id2\"],\n",
    "    where={\"source\":\"llamaindex.ai\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf7e73-d6ea-4cba-b1b3-ab2f290463eb",
   "metadata": {},
   "source": [
    "ChromaDB uses HNSW for its vector index. This can be tuned by specifying the `space` param that can have values `(l2, ip, cosine)`\n",
    "\n",
    "```python\n",
    "client.create_collection(\n",
    "    name=\"my_collection\",\n",
    "    metadata={\"description\",\"My Demo Collection\"},\n",
    "    configuration={\"hnsw\" : {\"space\":\"cosine\"}},\n",
    "    embedding_function=ef\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013f589-d710-41da-87f9-69ac07023c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lab - Similarity Search on Employee Records using Python and Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310f39c-e363-476d-8ad4-5ddde20f1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb==1.0.12\n",
    "!pip install sentence-transformers==4.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e9b8d-481f-4754-87bf-4aebc6ca46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules from the chromadb package:\n",
    "# chromadb is used to interact with the Chroma DB database,\n",
    "# embedding_functions is used to define the embedding model\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Define the embedding function using SentenceTransformers\n",
    "# This function will be used to generate embeddings (vector representations) for the data\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Creating an instance of ChromaClient to establish a connection with the Chroma database\n",
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535eb339-a3cf-49e5-a9b0-fecd62127c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a name for the collection where data will be stored or accessed\n",
    "# This collection is likely used to group related records, such as employee data\n",
    "collection_name = \"employee_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e995651f-e18b-438c-9354-39f0a48ce672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform various types of searches within the collection\n",
    "def perform_advanced_search(collection, all_items):\n",
    "    try:\n",
    "        # Advanced search operations will be placed here\n",
    "        print(\"=== Similarity Search Examples ===\")\n",
    "        \n",
    "        # Example 1: Search for Python developers\n",
    "        print(\"\\n1. Searching for Python developers:\")\n",
    "        query_text = \"Python developer with web development experience\"\n",
    "        results = collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=3\n",
    "        )\n",
    "        print(f\"Query: '{query_text}'\")\n",
    "        for i, (doc_id, document, distance) in enumerate(zip(\n",
    "            results['ids'][0], results['documents'][0], results['distances'][0]\n",
    "        )):\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            print(f\"  {i+1}. {metadata['name']} ({doc_id}) - Distance: {distance:.4f}\")\n",
    "            print(f\"     Role: {metadata['role']}, Department: {metadata['department']}\")\n",
    "            print(f\"     Document: {document[:100]}...\")\n",
    "        \n",
    "        # Example 2: Search for leadership roles\n",
    "        print(\"\\n2. Searching for leadership and management roles:\")\n",
    "        query_text = \"team leader manager with experience\"\n",
    "        results = collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=3\n",
    "        )\n",
    "        print(f\"Query: '{query_text}'\")\n",
    "        for i, (doc_id, document, distance) in enumerate(zip(\n",
    "            results['ids'][0], results['documents'][0], results['distances'][0]\n",
    "        )):\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            print(f\"  {i+1}. {metadata['name']} ({doc_id}) - Distance: {distance:.4f}\")\n",
    "            print(f\"     Role: {metadata['role']}, Experience: {metadata['experience']} years\")\n",
    "\n",
    "        print(\"\\n=== Metadata Filtering Examples ===\")\n",
    "\n",
    "        # Example 1: Filter by department\n",
    "        print(\"\\n3. Finding all Engineering employees:\")\n",
    "        results = collection.get(\n",
    "            where={\"department\": \"Engineering\"}\n",
    "        )\n",
    "        print(f\"Found {len(results['ids'])} Engineering employees:\")\n",
    "        for i, doc_id in enumerate(results['ids']):\n",
    "            metadata = results['metadatas'][i]\n",
    "            print(f\"  - {metadata['name']}: {metadata['role']} ({metadata['experience']} years)\")\n",
    "        \n",
    "        # Example 2: Filter by experience range\n",
    "        print(\"\\n4. Finding employees with 10+ years experience:\")\n",
    "        results = collection.get(\n",
    "            where={\"experience\": {\"$gte\": 10}}\n",
    "        )\n",
    "        print(f\"Found {len(results['ids'])} senior employees:\")\n",
    "        for i, doc_id in enumerate(results['ids']):\n",
    "            metadata = results['metadatas'][i]\n",
    "            print(f\"  - {metadata['name']}: {metadata['role']} ({metadata['experience']} years)\")\n",
    "        \n",
    "        # Example 3: Filter by location\n",
    "        print(\"\\n5. Finding employees in California:\")\n",
    "        results = collection.get(\n",
    "            where={\"location\": {\"$in\": [\"San Francisco\", \"Los Angeles\"]}}\n",
    "        )\n",
    "        print(f\"Found {len(results['ids'])} employees in California:\")\n",
    "        for i, doc_id in enumerate(results['ids']):\n",
    "            metadata = results['metadatas'][i]\n",
    "            print(f\"  - {metadata['name']}: {metadata['location']}\")\n",
    "\n",
    "        print(\"\\n=== Combined Search: Similarity + Metadata Filtering ===\")\n",
    "\n",
    "        # Example: Find experienced Python developers in specific locations\n",
    "        print(\"\\n6. Finding senior Python developers in major tech cities:\")\n",
    "        query_text = \"senior Python developer full-stack\"\n",
    "        results = collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=5,\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    {\"experience\": {\"$gte\": 8}},\n",
    "                    {\"location\": {\"$in\": [\"San Francisco\", \"New York\", \"Seattle\"]}}\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        print(f\"Query: '{query_text}' with filters (8+ years, major tech cities)\")\n",
    "        print(f\"Found {len(results['ids'][0])} matching employees:\")\n",
    "        \n",
    "        for i, (doc_id, document, distance) in enumerate(zip(\n",
    "            results['ids'][0], results['documents'][0], results['distances'][0]\n",
    "        )):\n",
    "            metadata = results['metadatas'][0][i]\n",
    "            print(f\"  {i+1}. {metadata['name']} ({doc_id}) - Distance: {distance:.4f}\")\n",
    "            print(f\"     {metadata['role']} in {metadata['location']} ({metadata['experience']} years)\")\n",
    "            print(f\"     Document snippet: {document[:80]}...\")\n",
    "\n",
    "        # Check if the results are empty or undefined\n",
    "        if not results or not results['ids'] or len(results['ids'][0]) == 0:\n",
    "            # Log a message if no similar documents are found for the query term\n",
    "            print(f'No documents found similar to \"{query_text}\"')\n",
    "            return\n",
    "\n",
    "        # Log the header for the top 3 similar documents based on the query term\n",
    "        print(f'Top 3 similar documents to \"{query_text}\":')\n",
    "        # Loop through the top 3 results and log the document details\n",
    "        for i in range(min(3, len(results['ids'][0]))):\n",
    "            # Extract the document ID and similarity score from the results\n",
    "            doc_id = results['ids'][0][i]\n",
    "            score = results['distances'][0][i]\n",
    "            # Retrieve the document text corresponding to the current ID from the results\n",
    "            text = results['documents'][0][i]\n",
    "            # Check if the text is available; if not, log 'Text not available'\n",
    "            if not text:\n",
    "                print(f' - ID: {doc_id}, Text: \"Text not available\", Score: {score:.4f}')\n",
    "            else:\n",
    "                print(f' - ID: {doc_id}, Text: \"{text}\", Score: {score:.4f}')\n",
    "            \n",
    "        pass\n",
    "    except Exception as error:\n",
    "        print(f\"Error in advanced search: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e26fed-3b1e-43d9-9231-735ea0775363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function named 'main'\n",
    "# This function is used to encapsulate the main operations for creating collections,\n",
    "# generating embeddings, and performing similarity search\n",
    "def main():\n",
    "    try:\n",
    "        # Code for database operations will be placed here\n",
    "        # This includes creating collections, adding data, and performing searches\n",
    "        # Creating a collection using the ChromaClient instance\n",
    "        # The 'create_collection' method creates a new collection with the specified configuration\n",
    "        collection = client.create_collection(\n",
    "            # Specifying the name of the collection to be created\n",
    "            name=collection_name,\n",
    "            # Adding metadata to describe the collection\n",
    "            metadata={\"description\": \"A collection for storing employee data\"},\n",
    "            # Configuring the collection with cosine distance and embedding function\n",
    "            configuration={\n",
    "                \"hnsw\": {\"space\": \"cosine\"},\n",
    "                \"embedding_function\": ef\n",
    "            }\n",
    "        )\n",
    "        print(f\"Collection created: {collection.name}\")\n",
    "\n",
    "        # Defining a list of employee dictionaries\n",
    "        # Each dictionary represents an individual employee with comprehensive information\n",
    "        employees = [\n",
    "            {\n",
    "                \"id\": \"employee_1\",\n",
    "                \"name\": \"John Doe\",\n",
    "                \"experience\": 5,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"Software Engineer\",\n",
    "                \"skills\": \"Python, JavaScript, React, Node.js, databases\",\n",
    "                \"location\": \"New York\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_2\",\n",
    "                \"name\": \"Jane Smith\",\n",
    "                \"experience\": 8,\n",
    "                \"department\": \"Marketing\",\n",
    "                \"role\": \"Marketing Manager\",\n",
    "                \"skills\": \"Digital marketing, SEO, content strategy, analytics, social media\",\n",
    "                \"location\": \"Los Angeles\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_3\",\n",
    "                \"name\": \"Alice Johnson\",\n",
    "                \"experience\": 3,\n",
    "                \"department\": \"HR\",\n",
    "                \"role\": \"HR Coordinator\",\n",
    "                \"skills\": \"Recruitment, employee relations, HR policies, training programs\",\n",
    "                \"location\": \"Chicago\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_4\",\n",
    "                \"name\": \"Michael Brown\",\n",
    "                \"experience\": 12,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"Senior Software Engineer\",\n",
    "                \"skills\": \"Java, Spring Boot, microservices, cloud architecture, DevOps\",\n",
    "                \"location\": \"San Francisco\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_5\",\n",
    "                \"name\": \"Emily Wilson\",\n",
    "                \"experience\": 2,\n",
    "                \"department\": \"Marketing\",\n",
    "                \"role\": \"Marketing Assistant\",\n",
    "                \"skills\": \"Content creation, email marketing, market research, social media management\",\n",
    "                \"location\": \"Austin\",\n",
    "                \"employment_type\": \"Part-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_6\",\n",
    "                \"name\": \"David Lee\",\n",
    "                \"experience\": 15,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"Engineering Manager\",\n",
    "                \"skills\": \"Team leadership, project management, software architecture, mentoring\",\n",
    "                \"location\": \"Seattle\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_7\",\n",
    "                \"name\": \"Sarah Clark\",\n",
    "                \"experience\": 8,\n",
    "                \"department\": \"HR\",\n",
    "                \"role\": \"HR Manager\",\n",
    "                \"skills\": \"Performance management, compensation planning, policy development, conflict resolution\",\n",
    "                \"location\": \"Boston\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_8\",\n",
    "                \"name\": \"Chris Evans\",\n",
    "                \"experience\": 20,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"Senior Architect\",\n",
    "                \"skills\": \"System design, distributed systems, cloud platforms, technical strategy\",\n",
    "                \"location\": \"New York\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_9\",\n",
    "                \"name\": \"Jessica Taylor\",\n",
    "                \"experience\": 4,\n",
    "                \"department\": \"Marketing\",\n",
    "                \"role\": \"Marketing Specialist\",\n",
    "                \"skills\": \"Brand management, advertising campaigns, customer analytics, creative strategy\",\n",
    "                \"location\": \"Miami\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_10\",\n",
    "                \"name\": \"Alex Rodriguez\",\n",
    "                \"experience\": 18,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"Lead Software Engineer\",\n",
    "                \"skills\": \"Full-stack development, React, Python, machine learning, data science\",\n",
    "                \"location\": \"Denver\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_11\",\n",
    "                \"name\": \"Hannah White\",\n",
    "                \"experience\": 6,\n",
    "                \"department\": \"HR\",\n",
    "                \"role\": \"HR Business Partner\",\n",
    "                \"skills\": \"Strategic HR, organizational development, change management, employee engagement\",\n",
    "                \"location\": \"Portland\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_12\",\n",
    "                \"name\": \"Kevin Martinez\",\n",
    "                \"experience\": 10,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"DevOps Engineer\",\n",
    "                \"skills\": \"Docker, Kubernetes, AWS, CI/CD pipelines, infrastructure automation\",\n",
    "                \"location\": \"Phoenix\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_13\",\n",
    "                \"name\": \"Rachel Brown\",\n",
    "                \"experience\": 7,\n",
    "                \"department\": \"Marketing\",\n",
    "                \"role\": \"Marketing Director\",\n",
    "                \"skills\": \"Strategic marketing, team leadership, budget management, campaign optimization\",\n",
    "                \"location\": \"Atlanta\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_14\",\n",
    "                \"name\": \"Matthew Garcia\",\n",
    "                \"experience\": 3,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"Junior Software Engineer\",\n",
    "                \"skills\": \"JavaScript, HTML/CSS, basic backend development, learning frameworks\",\n",
    "                \"location\": \"Dallas\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"employee_15\",\n",
    "                \"name\": \"Olivia Moore\",\n",
    "                \"experience\": 12,\n",
    "                \"department\": \"Engineering\",\n",
    "                \"role\": \"Principal Engineer\",\n",
    "                \"skills\": \"Technical leadership, system architecture, performance optimization, mentoring\",\n",
    "                \"location\": \"San Francisco\",\n",
    "                \"employment_type\": \"Full-time\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Create comprehensive text documents for each employee\n",
    "        # These documents will be used for similarity search based on skills, roles, and experience\n",
    "        employee_documents = []\n",
    "        for employee in employees:\n",
    "            document = f\"{employee['role']} with {employee['experience']} years of experience in {employee['department']}. \"\n",
    "            document += f\"Skills: {employee['skills']}. Located in {employee['location']}. \"\n",
    "            document += f\"Employment type: {employee['employment_type']}.\"\n",
    "            employee_documents.append(document)\n",
    "\n",
    "        # Adding data to the collection in the Chroma database\n",
    "        # The 'add' method inserts or updates data into the specified collection\n",
    "        collection.add(\n",
    "            # Extracting employee IDs to be used as unique identifiers for each record\n",
    "            ids=[employee[\"id\"] for employee in employees],\n",
    "            # Using the comprehensive text documents we created\n",
    "            documents=employee_documents,\n",
    "            # Adding comprehensive metadata for filtering and search\n",
    "            metadatas=[{\n",
    "                \"name\": employee[\"name\"],\n",
    "                \"department\": employee[\"department\"],\n",
    "                \"role\": employee[\"role\"],\n",
    "                \"experience\": employee[\"experience\"],\n",
    "                \"location\": employee[\"location\"],\n",
    "                \"employment_type\": employee[\"employment_type\"]\n",
    "            } for employee in employees]\n",
    "        )\n",
    "\n",
    "        # Retrieving all items from the specified collection\n",
    "        # The 'get' method fetches all records stored in the collection\n",
    "        all_items = collection.get()\n",
    "        # Logging the retrieved items to the console for inspection or debugging\n",
    "        print(\"Collection contents:\")\n",
    "        print(f\"Number of documents: {len(all_items['documents'])}\")\n",
    "\n",
    "        # Call the perform_advanced_search function with the collection and all_items as arguments\n",
    "        perform_advanced_search(collection, all_items)\n",
    "        \n",
    "        pass\n",
    "    except Exception as error:\n",
    "        # Catching and handling any errors that occur within the 'try' block\n",
    "        # Logs the error message to the console for debugging purposes\n",
    "        print(f\"Error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049cb51-3d4f-4a1a-85fe-654137fcd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed30cd7-8af5-4f3a-9c44-63ec1bd0db70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 2 - How Vector Databases power RAG "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307c91a-ea94-4606-b486-e9e9082eb44b",
   "metadata": {},
   "source": [
    "The RAG Pipeline and role of vector databases\n",
    "\n",
    "<img src=\"images/RAG_Pipeline.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3e890-6675-4358-87cb-90612658f3fa",
   "metadata": {},
   "source": [
    "**Why Vector DB for RAG**\n",
    "\n",
    "- **Reduces risk** of critical errors such as\n",
    "    - using different embedding models for source documents and user prompts\n",
    "    - incorrectly linking embeddings to their source documents\n",
    "- **Simplifies and speeds up** development\n",
    "    - less custom code to maintain\n",
    "    - faster implementation and debugging\n",
    "- **Optimizes Performance**\n",
    "    - Vector DBs are built for fast semantic similarity searches\n",
    "    - Custom built alternatives are often slower without significant optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa40a6-6714-4075-8675-0de1686ad0f2",
   "metadata": {},
   "source": [
    "**Pitfalls of RAG**\n",
    "\n",
    "|Pitfall|Remedy|\n",
    "|------|------|\n",
    "|Using different embedding models for source docs and prompts|Use same embedding model everywhere (vector DBs do this by default)|\n",
    "|Chunks are either too short or too long|Choose chunk size long enough to keep meaning clear|\n",
    "|Not re-embedding when data, metrics or embedding model changes|re-embed when data, metrics or model changes|\n",
    "|Assuming retrieval is always accurate|Test results and adjust if needed|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212b9f5-d77d-474f-8c3e-35ee8140ddfe",
   "metadata": {},
   "source": [
    "RAG frameworks like LangChain and LlamaIndex handle things that Vector DBs don't\n",
    "- chunking\n",
    "- advanced retrieval logic\n",
    "- prompt augmentation\n",
    "- LLM integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43001e4-c857-49b9-972b-df6d86c2a337",
   "metadata": {},
   "source": [
    "#### Lab - Food Recommendation system using Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0fd01-308a-4f80-9808-965ade986038",
   "metadata": {},
   "source": [
    "See Code/ibm-genai-course/food-chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f06782-ca05-47a4-9b14-57da5f6451ba",
   "metadata": {},
   "source": [
    "## Course 4 - Advanced RAG with Vector Databases and Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5818e22-49b6-435c-b5b5-b1c3384d1f8f",
   "metadata": {},
   "source": [
    "### Module 1 - Advanced Retrievers for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea29307-cd81-42aa-b8ca-14471a1f3b65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 1 - Explore Advanced Retrievers in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9912e44-87c6-4684-9c29-393666e937d4",
   "metadata": {},
   "source": [
    "**What is a Langchain Retriever**\n",
    "- An interface that returns documents based on an unstructured query\n",
    "- More general than a vector store\n",
    "- Retrieves documents or their chunks\n",
    "- Accepts a string query as input and returns a list of documents or chunks as output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3fda2-9fbe-4b20-9a47-5dfe7460a8f2",
   "metadata": {},
   "source": [
    "**Vector Store Based Retriever**\n",
    "- It plugs into an existing vector store and works by embedding the query and then comparing them with embedded chunks using similarity search or **maximum marginal relevance (MMR)** to retrieve the most relevant chunks.\n",
    "- It is simple as it plugs into an existing vector DB and does not require LLM to retrieve most similar chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be5705-bce7-4a37-9187-101648568dc0",
   "metadata": {},
   "source": [
    "**MMR**\n",
    "- MMR is a technique to improve both relevance and diversity of retrieved results\n",
    "    - MMR selects documents that are most relevant to query and also minimally similar to previous documents\n",
    "    - This avoids redundancy and ensures comprehensive coverage for the query\n",
    "```python\n",
    "retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    "docs=retriever.invoke(\"email policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3c960-6c8b-43e4-8598-e4c97a9aace0",
   "metadata": {},
   "source": [
    "**Multi-Query Retriever**\n",
    "- Uses an LLM to generate different versions of the same query\n",
    "- this overcomes differences in results due to changed in query wording or poor embeddings\n",
    "- Takes a union across all documents returned by all query variants to build a larger set\n",
    "\n",
    "*Example*\n",
    "```python\n",
    "\n",
    "from ibm_watson_ai.foundation_models import ModelInference\n",
    "from ibm_watson_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_ai import Credentials\n",
    "from ibm_watson_ai.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "def llm():\n",
    "    model_id = \"mistalai/mixtral-8x7b-instruct-v01\"\n",
    "    params = {\n",
    "        GenParams.MAX_NEW_TOKENS: 256,\n",
    "        GenParams.TEMPERATURE: 0.5\n",
    "    }\n",
    "    credentials = \"https://us-south.ml.cloud.ibm.com\"\n",
    "    project_id=\"skills-network\"\n",
    "    model = ModelInference(\n",
    "        model_id=model_id,\n",
    "        params=params,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    mixtral_llm=WatsonxLLM(model=model)\n",
    "    return mixtral_llm\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# The multi query retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    llm = llm()\n",
    ")\n",
    "\n",
    "docs = retriever.invoke(\"email policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3e283-133a-4e42-914c-1ba1d619f7d1",
   "metadata": {},
   "source": [
    "**Self-Query Retriever**\n",
    "- Documents have text as well as metadata.\n",
    "- None of the retrievers discussed so far have the ability to access the metadata\n",
    "- Converts every query into\n",
    "    1. A string to look up semantically and\n",
    "    2. Metadata filter to go along with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c033494-7870-4cae-a5aa-d058add6ab93",
   "metadata": {},
   "source": [
    "**Self-Query retriever setup**\n",
    "```python\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrivers.self_query.base import SelfQueryRetriever\n",
    "from lark import lark\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\",1993, \"rating\" : 7.7, \"genre\": \"science fiction\"}\n",
    "    )\n",
    "    #... and  many more documents ...\n",
    "]\n",
    "\n",
    "vectordb=Chroma.from_document(docs, watsonx_embeddings)\n",
    "\n",
    "metadata_field_info=[\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama']\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\",\n",
    "        description=\"A 1-10 rating of the movie\",\n",
    "        type=\"float\"\n",
    "    )\n",
    "]\n",
    "\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "\n",
    "retriever = SeflQueryRetriever.from_llm(\n",
    "    llm(),\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info\n",
    ")\n",
    "\n",
    "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ac25-760a-4165-a53c-d20e7b5cee39",
   "metadata": {},
   "source": [
    "**Parent Document Retriever**\n",
    "- Splitting documents involves conflict between small documents for accuracy and long documents for context\n",
    "- Parent Document Retriever fetches small chunks, looks up their parent IDs and then returns large documents for the small chunks\n",
    "- in the setup below, after `invoke`the retriever will return large chunks matched to the smaller chunks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068a82e-4126-48b4-86ae-41fb627e5bca",
   "metadata": {},
   "source": [
    "**Parent Document retriever setup**\n",
    "```python\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.text_splitters import CharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "#setup two splitters, one with big chunk size (parent) and one with small chunk size (child)\n",
    "parent_splitter=CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator=\"\\n\")\n",
    "child_splitter=CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator=\"\\n\")\n",
    "\n",
    "vectordb=Chroma(\n",
    "    collection_name=\"split_parents\",\n",
    "    embedding_function=watsonx_embedding\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "retriever=ParentDocumentRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "retriever.add_documents(data)\n",
    "retriever.invoke(\"smoking policy\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e8986-7041-428f-ad4c-c0cc25475cbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lab 1 - Build a Smarter Search with LangChain Context Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3084e6-2e5e-4d85-b113-a6607c7c3217",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install \"ibm-watsonx-ai==1.1.2\" | tail -n 1\n",
    "!{sys.executable} -m pip install \"langchain==0.2.1\" | tail -n 1\n",
    "!{sys.executable} -m pip install \"langchain-ibm==0.1.11\" | tail -n 1\n",
    "!{sys.executable} -m pip install \"langchain-community==0.2.1\" | tail -n 1\n",
    "!{sys.executable} -m pip install \"chromadb==0.4.24\" | tail -n 1\n",
    "!{sys.executable} -m pip install \"pypdf==4.3.1\" | tail -n 1\n",
    "!{sys.executable} -m pip install \"lark==1.1.9\" | tail -n 1\n",
    "!{sys.executable} -m pip install 'posthog<6.0.0' | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb9223-3d2c-4cd8-96f2-3f450d79455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66041c9f-6a5e-4044-9fd6-0f305caa1aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135551bc-c103-4a21-beb5-804d8f620901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm():\n",
    "    model_id = 'mistralai/mistral-small-3-1-24b-instruct-2503'\n",
    "    \n",
    "    parameters = {\n",
    "        GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "        GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "    }\n",
    "    \n",
    "    credentials = {\n",
    "        \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    project_id = \"skills-network\"\n",
    "    \n",
    "    model = ModelInference(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    mixtral_llm = WatsonxLLM(model = model)\n",
    "    return mixtral_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1ae45-57d2-4ba2-b0dd-3ddab755df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aaa3ec-838a-40a3-943e-6d0336636401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(data, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694b163-af93-43ee-9a65-9ecc1c7d6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2328fb-644a-450e-8e0f-ac7bd6bcdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def watsonx_embedding():\n",
    "    embed_params = {\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "    }\n",
    "    \n",
    "    watsonx_embedding = WatsonxEmbeddings(\n",
    "        model_id=\"ibm/slate-125m-english-rtrvr-v2\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        params=embed_params,\n",
    "    )\n",
    "    return watsonx_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7993793-53e6-45d7-8aa6-02f720dac838",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MZ9z1lm-Ui3YBp3SYWLTAQ/companypolicies.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a38df-326e-48a1-a87f-cf50720b35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b989f-22b5-4130-a1e8-59f7e0a58832",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"companypolicies.txt\")\n",
    "txt_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390c3f1-a6b5-43fc-a359-beaa80add8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_txt = text_splitter(txt_data, 200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c0375-cb2f-47fe-95b5-598eb79c22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666c79d-a4e4-46ed-bd1e-d0ec497bab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will fail right now because Watson API key is needed outside of the Skills Network Notebooks\n",
    "vectordb = Chroma.from_documents(chunks_txt, watsonx_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea4a1f-5407-4cfb-96c5-99f96aff378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"email policy\"\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca2d51-0000-4888-bba0-5fd9286b023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also specify search kwargs like k to limit the retrieval results\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467fb9c-ac09-45cc-a31a-85e920eca151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is showing how to conduct an MMR search in a vector database. \n",
    "# You just need to sepecify search_type=\"mmr\".\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680057c-a7de-4ca0-a7d6-b68b1e4efbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also set a retrieval method that defines a similarity score threshold, \n",
    "# returning only documents with a score above that threshold.\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e9adb-c1de-47ff-97f7-c5781e837cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a1295-0e02-4842-baab-a295651c015d",
   "metadata": {},
   "source": [
    "**Multi-Query Retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca7140-8914-430c-b909-9a5af0db5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ioch1wsxkfqgfLLgmd-6Rw/langchain-paper.pdf\")\n",
    "pdf_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47599b2b-a176-422e-ac37-6f04b6d4f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document and store the embeddings into a vector database.\n",
    "\n",
    "# Split\n",
    "chunks_pdf = text_splitter(pdf_data, 500, 20)\n",
    "\n",
    "# VectorDB\n",
    "ids = vectordb.get()[\"ids\"]\n",
    "vectordb.delete(ids) # We need to delete existing embeddings from previous documents and then store current document embeddings in.\n",
    "vectordb = Chroma.from_documents(documents=chunks_pdf, embedding=watsonx_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193be06-cb26-46c4-9d9e-a87c6bfeb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MultiQueryRetriever function from LangChain is used.\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "query = \"What does the paper say about langchain?\"\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b7e0c-12df-4359-a1ff-e8f2f7961dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries.\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcdd61-e35a-4e64-9e60-546778f6abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2989b0-4865-4ab6-a5a1-de604213dfa1",
   "metadata": {},
   "source": [
    "**Self-Querying Retriever**\n",
    "\n",
    "A Self-Querying Retriever, as the name suggests, has the ability to query itself. Specifically, given a natural language query, the retriever uses a query-constructing LLM chain to generate a structured query. It then applies this structured query to its underlying vector store. This enables the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but also to extract and apply filters based on the metadata of those documents.\n",
    "The following code demonstrates how to use a Self-Querying Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5344e46-c574-464c-aef0-222b826cf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from lark import lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46739fb-4833-4ee6-9499-a646c42a9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aec84-4599-436f-b4ad-db4e7bd04d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12968202-2940-412f-9c17-033a35dfede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(docs, watsonx_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68dafef-73e4-4cd2-b09a-30404af68f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Brief summary of a movie.\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm(),\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db09c7-584f-4a9d-96e3-203dc60e72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example only specifies a filter\n",
    "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee166e39-73e8-4b26-91db-cbfc74e022ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example specifies a query and a filter\n",
    "retriever.invoke(\"Has Greta Gerwig directed any movies about women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16709ab1-4630-46d1-b6ea-d4bc6bccc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example specifies a composite filter\n",
    "retriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a724b-d1e5-4ecb-83f9-d7114bf1ca6d",
   "metadata": {},
   "source": [
    "**Parent Document Retriever**\n",
    "\n",
    "When splitting documents for retrieval, there are often conflicting desires:\n",
    "\n",
    "1. You may want to have small documents so that their embeddings can most accurately reflect their meaning. If the documents are too long, the embeddings can lose meaning.\n",
    "2. You want to have long enough documents so that the context of each chunk is retained.\n",
    "\n",
    "The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent IDs for those chunks and returns those larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d472e80-1947-4c10-9b1f-bace9372e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f3fd9-a293-4c11-81cb-51df645fc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20, separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b47b55-11f3-4392-ac81-1d8363c0e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=watsonx_embedding()\n",
    ")\n",
    "#vectordb = Chroma.from_documents(documents=chunks_pdf, embedding=watsonx_embedding())\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d161eca2-a99a-42e4-89f7-91901ff3ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c5ca8-5235-4d5b-8b59-3ccead766573",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(txt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb9f19-3b6b-45d7-982b-b0b46acc21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the number of large chunks:\n",
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b50ac-9444-4968-ae94-ae6b6f1ca868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure the underlying vector store still retrieves the small chunks.\n",
    "sub_docs = vectordb.similarity_search(\"smoking policy\")\n",
    "print(sub_docs[0].page_content)\n",
    "retrieved_docs = retriever.invoke(\"smoking policy\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b94d2f-57e2-4f94-856d-9d134991027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 1: Retrieve the top two results for the company policy document for the query \"smoking policy\" \n",
    "# using the Vector Store-Backed Retriever.\n",
    "vectordb = Chroma.from_documents(documents=chunks_txt, embedding=watsonx_embedding())\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
    "query = \"smoking policy\"\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddecbf2-afb0-4ff6-a19e-3b597048b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 2: Use the Self-Querying Retriever to invoke a query with a filter.\n",
    "\n",
    "# You might encouter some errors or blank content when run the following code.\n",
    "# It is becasue LLM cannot get the answer at first. Don't worry, re-run it several times you will get the answer.\n",
    "\n",
    "vectordb = Chroma.from_documents(docs, watsonx_embedding())\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm(),\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")\n",
    "\n",
    "# This example specifies a query with filter\n",
    "retriever.invoke(\n",
    "    \"I want to watch a movie directed by Christopher Nolan\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc821eb1-01af-47c4-aa57-dfe6d1f13c85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 2 - Advanced Retrievers in LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ccdd2-1c42-4be9-9225-84e041496097",
   "metadata": {},
   "source": [
    "Advanced retrievers in LlamaIndex are sophisticated components that go beyond simple vector similarity search to provide more nuanced, context-aware, and intelligent information retrieval. They combine multiple techniques such as:\n",
    "\n",
    "- **Semantic Understanding**: Using embeddings to understand meaning and context\n",
    "- **Keyword Matching**: Precise term-based search for exact specifications\n",
    "- **Hierarchical Context**: Maintaining relationships between different levels of information\n",
    "- **Multi-Query Processing**: Generating and combining results from multiple query variations\n",
    "- **Fusion Techniques**: Intelligently combining results from different retrieval methods\n",
    "\n",
    "Why are Advanced Retrievers Important?\n",
    "\n",
    "- **Improved Accuracy**: Advanced retrievers can find more relevant information by using multiple search strategies\n",
    "- **Better Context Preservation**: They maintain important relationships between pieces of information\n",
    "- **Reduced Hallucination**: More precise retrieval leads to more accurate AI responses\n",
    "- **Scalability**: Efficient retrieval strategies work better with large document collections\n",
    "- **Flexibility**: Different retrieval methods can be combined for optimal results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe227d14-8983-467f-974e-9b2379b6b63c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Types of Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ff1ea-8a34-4ca0-9540-44a8a30b9a86",
   "metadata": {},
   "source": [
    "**VectorStoreIndex**\n",
    "\n",
    "Enables semantic search based on meaning\n",
    "- Stores embeddings for each document chunk\n",
    "- Best for semantic retrieval\n",
    "- Common in LLM pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a1de0-1ee3-41a0-8dd1-ad05ccc31db7",
   "metadata": {},
   "source": [
    "**DocumentSummaryIndex**\n",
    "\n",
    "Generated Summaries to identify relevant documents\n",
    "- Generates and stores summaries of documents\n",
    "- Filter Documents before full retrieval\n",
    "- Useful for large, diverse document sets (that cannot fit in LLM context window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d0a76c-344a-4ade-9b04-07cd3a43ab18",
   "metadata": {},
   "source": [
    "**KeywordTableIndex**\n",
    "\n",
    "Exact keyword matching for rule-based or hybrid search\n",
    "- Extracts keywords from documents\n",
    "- Maps keywords to specific chunks of content\n",
    "- Enables exact keyword matching\n",
    "- Useful for hybrid or rule-based search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d53d03-a2fb-4e40-8964-4ce8ede4686e",
   "metadata": {},
   "source": [
    "##### Core and Advanced Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38738b5b-0ba8-4045-9e11-c5ba07baf61f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **Vector Index Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547a243-e601-49cb-be7e-7fe89769a025",
   "metadata": {},
   "source": [
    "- uses embeddings to find relevant content\n",
    "- ideal for general purpose search\n",
    "- common in RAG\n",
    "\n",
    "The Vector Index Retriever uses vector embeddings to find semantically related content, making it ideal for general-purpose search and widely used in retrieval-augmented generation (RAG) pipelines.\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "- Documents are split into nodes and embedded using the configured embedding model\n",
    "- Query is converted to an embedding vector\n",
    "- Returns nodes ranked by cosine similarity to the query embedding\n",
    "- Generates embeddings in batches of 2048 nodes by default\n",
    "\n",
    "**When to use**:\n",
    "\n",
    "- General-purpose semantic search (most common use case)\n",
    "- Finding conceptually related content based on meaning rather than exact keywords\n",
    "- RAG pipelines where semantic understanding is crucial\n",
    "- When exact keyword matching isn't the primary requirement\n",
    "\n",
    "**Key characteristics from authoritative source**:\n",
    "\n",
    "- Stores embeddings for each document chunk (VectorStoreIndex foundation)\n",
    "- Best for semantic retrieval based on meaning and context\n",
    "- Commonly used in LLM pipelines for retrieval-augmented generation\n",
    "\n",
    "**Strengths**:\n",
    "\n",
    "- Excellent semantic understanding and context awareness\n",
    "- Handles synonyms and related concepts effectively\n",
    "- Works well with natural language queries\n",
    "\n",
    "**Limitations**:\n",
    "\n",
    "- May miss exact keyword matches when specific terms are crucial\n",
    "- Requires a good embedding model for optimal performance\n",
    "- Can be computationally intensive for large document collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08f9ea-b7cd-4687-bb5a-33931f87ef2a",
   "metadata": {},
   "source": [
    ">TF-IDF : \n",
    "> **Term Frequency** (Word frequency in a document) X **Inverse Document Frequency** (how rare that word is across all documents)\n",
    "> Thus it highlights words that are frequent in one document but rare across all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274b8be-f072-40ab-a00e-0f15ded05b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **BM-25 Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab14c7-ede2-4ded-9c2a-7d2c7838f1a6",
   "metadata": {},
   "source": [
    "- Keyword based retrieval for ranking documents\n",
    "- Retrieves content based on exact keyword matches (not semantic similarity)\n",
    "- Improves on TF-IDF by\n",
    "    - Adding term frequency saturation\n",
    "    - Normalizing for document length\n",
    "\n",
    "BM25 is a keyword-based retrieval method that improves on TF-IDF by addressing some of its key limitations. It's widely used in production search systems including Elasticsearch and Apache Lucene.\n",
    "\n",
    ">**Understanding TF-IDF: The Foundation**\n",
    ">\n",
    ">Before diving into BM25, let's understand TF-IDF (Term Frequency-Inverse Document Frequency), which BM25 builds upon:\n",
    ">\n",
    ">Term Frequency (TF): Measures how often a word appears in a document\n",
    ">\n",
    ">Example: If \"neural\" appears 3 times in a 100-word document, TF = 3/100 = 0.03\n",
    ">\n",
    ">Inverse Document Frequency (IDF): Measures how rare a word is across all documents\n",
    ">\n",
    ">Example: If \"neural\" appears in only 2 out of 1000 documents, IDF = log(1000/2) = 6.21\n",
    ">\n",
    ">*Common words like \"the\" have low IDF; rare technical terms have high IDF*\n",
    ">\n",
    ">TF-IDF Score: TF × IDF\n",
    ">\n",
    ">Highlights words that are frequent in one document but rare across the collection\n",
    ">\n",
    ">Developed by Karen Spärck Jones, who pioneered the concept of term specificity\n",
    "\n",
    "**How BM25 Improves Upon TF-IDF**\n",
    "\n",
    "Key BM25 Improvements:\n",
    "\n",
    "**Term Frequency Saturation**: BM25 reduces the impact of repeated terms using term frequency saturation\n",
    "\n",
    "Problem: In TF-IDF, if a word appears 100 times vs 10 times, the score increases linearly\n",
    "\n",
    "Solution: BM25 uses a saturation function that plateaus after a certain frequency\n",
    "\n",
    "**Document Length Normalization**: BM25 adjusts for document length, making it more effective for keyword-based search\n",
    "\n",
    "Problem: In TF-IDF, longer documents have unfair advantages\n",
    "\n",
    "Solution: BM25 normalizes scores based on document length relative to average\n",
    "\n",
    "Tunable Parameters: Allows fine-tuning for different types of content\n",
    "\n",
    "k1 ≈ 1.2: Controls term frequency saturation (how quickly scores plateau)\n",
    "\n",
    "b ≈ 0.75: Controls document length normalization (0=none, 1=full)\n",
    "\n",
    "**When to Use BM25**\n",
    "\n",
    "Ideal for:\n",
    "\n",
    "- Technical documentation where exact terms matter\n",
    "- Legal documents with specific terminology\n",
    "- Product catalogs with precise specifications\n",
    "- Academic papers with specialized vocabulary\n",
    "- Applications requiring keyword-based retrieval rather than semantic similarity\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- Excellent precision for exact term matches\n",
    "- Fast computational performance\n",
    "- Proven effectiveness in production systems\n",
    "- No training required (unlike neural approaches)\n",
    "- Interpretable scoring mechanism\n",
    "\n",
    "**Limitations**:\n",
    "\n",
    "- No semantic understanding (doesn't handle synonyms)\n",
    "- Struggles with typos and variations\n",
    "- Limited context understanding\n",
    "- Requires careful parameter tuning for optimal performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbce199-08fe-4838-abe7-0b397e28541c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **Document Summary Index Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cdc30-7396-45b8-bc00-98ee052535e2",
   "metadata": {},
   "source": [
    "- use summaries to filter documents\n",
    "- Two versions\n",
    "    - LLM based (time-consuming, expensive)\n",
    "    - Embedding based (uses semantic similarity, efficient for large collections)\n",
    "- returns original documents, not their summaries\n",
    "\n",
    "Document Summary Index Retrievers use document summaries instead of the actual documents to find relevant content, making them efficient for large collections. They return the original documents, not their summaries.\n",
    "\n",
    "**How it works (from authoritative source)**:\n",
    "\n",
    "- Generates and stores summaries of documents at indexing time\n",
    "- Uses summaries to filter documents before retrieving full content\n",
    "- Two-stage Process: First uses summaries to filter documents, then returns full document content\n",
    "- Especially useful for large, diverse corpora that cannot fit in the context window of an LLM\n",
    "\n",
    "**Two Retrieval Options**:\n",
    "\n",
    "**DocumentSummaryIndexLLMRetriever**:\n",
    "\n",
    "- Uses a large language model to analyze the query against document summaries\n",
    "- Provides intelligent document selection but can be more time-consuming and expensive\n",
    "- Best for complex queries requiring nuanced understanding\n",
    "\n",
    "**DocumentSummaryIndexEmbeddingRetriever**:\n",
    "\n",
    "- Uses semantic similarity between the query and summary embeddings\n",
    "- Faster and more cost-effective than LLM-based approach\n",
    "- Good for straightforward similarity matching\n",
    "\n",
    "**When to use (based on authoritative guidance)**:\n",
    "\n",
    "- Large document collections where documents cover different topics\n",
    "- When you need efficient document-level filtering before detailed retrieval\n",
    "- Multi-document QA where documents have distinct subject matters\n",
    "- Large and diverse document sets that cannot fit in the context window of an LLM\n",
    "\n",
    "**Configuration Parameters**:\n",
    "\n",
    "- choice_top_k (LLM retriever): Number of documents to select\n",
    "- similarity_top_k (Embedding retriever): Number of documents to select\n",
    "- Default is 1, increase for multiple document retrieval\n",
    "\n",
    "**Key Point**: Returns original documents, not their summaries - the summaries are only used for filtering\n",
    "\n",
    "**Strengths**:\n",
    "\n",
    "- Efficient document selection and reduces search space\n",
    "- Good for heterogeneous collections with diverse topics\n",
    "- Returns original documents with full context intact\n",
    "\n",
    "**Limitations**:\n",
    "\n",
    "- Requires LLM for summary generation during indexing\n",
    "- May lose some detail present in original documents during summary creation\n",
    "- LLM-based version can be slower and more expensive than other options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18794e3-0443-49d6-abc1-c28507d918d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **Auto Merging Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c93a36-0759-4260-8cf9-5a63c735bcb7",
   "metadata": {},
   "source": [
    "- preserves context in long docs using a hierarchical structure\n",
    "- uses hierarchical chunking to break document into parent and child nodes\n",
    "- Retrieves parent node if enough child nodes match\n",
    "- Consolidates related content and preserves broader context\n",
    "\n",
    "Auto Merging Retriever is designed to preserve context in long documents using a hierarchical structure. **It uses hierarchical chunking to break documents into parent and child nodes, and if enough child nodes from the same parent are retrieved, the retriever returns the parent node instead.**\n",
    "\n",
    "**How it works (from authoritative source)**:\n",
    "- **Uses hierarchical chunking** to break documents into parent and child nodes\n",
    "- **Retrieves parent if enough children match** - intelligent merging logic\n",
    "- **Preserves context in long documents** by consolidating related content\n",
    "- **Dual Storage**: Smaller child chunks are indexed in the vector store for precise matching, while larger parent chunks are stored in the docstore\n",
    "\n",
    "**Key behavior pattern**:\n",
    "- Child chunks enable precise matching for specific queries\n",
    "- When multiple child chunks from the same parent are retrieved, the system returns the parent chunk\n",
    "- This **helps consolidate related content and preserve broader context**\n",
    "\n",
    "**When to use (based on authoritative guidance):**\n",
    "- Long documents where small chunks lose important surrounding context\n",
    "- Legal documents, research papers, technical specifications that need context preservation\n",
    "- When you need both precise matching and comprehensive context\n",
    "- Documents with natural hierarchical structure (sections, subsections)\n",
    "\n",
    "**Configuration:**\n",
    "- `chunk_sizes`: List of chunk sizes from largest to smallest (e.g., [512, 256, 128])\n",
    "- `chunk_overlap`: Overlap between chunks to maintain continuity\n",
    "- Storage context manages both vector store (child nodes) and docstore (parent nodes)\n",
    "\n",
    "**Strengths**: \n",
    "- Automatically preserves context without manual intervention\n",
    "- Reduces information fragmentation in long documents\n",
    "- Intelligent merging based on retrieval patterns\n",
    "- Maintains granular search capability while providing broader context\n",
    "\n",
    "**Limitations**: \n",
    "- More complex setup compared to basic retrievers\n",
    "- Requires hierarchical document structure to be effective\n",
    "- Higher storage overhead due to multiple chunk levels\n",
    "- May not be suitable for very short documents\n",
    "\n",
    "*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d9c0f-d1d3-489e-b68c-026ccaa13974",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **Recursive Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce9b80-8517-4e4b-9ca7-835d0d02be17",
   "metadata": {},
   "source": [
    "- Follows node relationships using references e.g.\n",
    "    - citations\n",
    "    - metadata links\n",
    "- Supports chunk and metadata linking references\n",
    "- Retrieves related content across documents or abstraction layers\n",
    "\n",
    "The Recursive Retriever is **designed to follow relationships between nodes using references**. **It can follow references from one node to another, such as citations in academic papers or other metadata links**, allowing it to **retrieve related content across documents or layers of abstraction**.\n",
    "\n",
    "**How it works (from authoritative source)**:\n",
    "- **Follows node references** - traverses relationships to find referenced content\n",
    "- **Supports chunk and metadata linking** - handles different types of references\n",
    "- **Multi-Level Navigation**: Can execute sub-queries on referenced retrievers or query engines\n",
    "- **Network Building**: Creates a network of interconnected retrievers that can reference each other\n",
    "\n",
    "**Reference Types Supported**:\n",
    "1. **Chunk References**: Smaller child chunks refer to larger parent chunks for additional context\n",
    "2. **Metadata References**: Summaries or generated questions refer to larger content chunks, such as citations in academic papers\n",
    "\n",
    "**When to use (based on authoritative guidance):**\n",
    "- **Academic papers with citations** and extensive references\n",
    "- **Research papers** where you need to retrieve relevant content from cited papers\n",
    "- Documentation with cross-references and linked content\n",
    "- Knowledge bases with interconnected information\n",
    "- When nodes reference structured data (tables, databases, other documents)\n",
    "\n",
    "**Configuration:**\n",
    "- `retriever_dict`: Maps node IDs or keys to specific retrievers\n",
    "- `query_engine_dict`: Maps keys to query engines for sub-queries\n",
    "- Node metadata can contain references to other nodes or data structures\n",
    "\n",
    "**Key capability**: **Retrieves related content across documents** by following reference chains\n",
    "\n",
    "**Strengths**: \n",
    "- Follows complex relationships and enables multi-step reasoning\n",
    "- Provides comprehensive coverage across related documents\n",
    "- Excellent for handling interconnected information systems\n",
    "- Can traverse multiple levels of references automatically\n",
    "\n",
    "**Limitations**: \n",
    "- Requires careful setup of node relationships\n",
    "- Can be computationally expensive for deep reference chains\n",
    "- Complex debugging when reference chains are extensive\n",
    "- May retrieve too much related content if not properly configured\n",
    "\n",
    "*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/recurisve_retriever_nodes_braintrust/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66033401-7587-4fdd-a981-7170f0fe501f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### **Query Fusion Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f7324-8d21-4c18-9c7d-ae5d71559dea",
   "metadata": {},
   "source": [
    "- Combines results from multiple different retrievers\n",
    "- Supports multiple query variations\n",
    "- Use fusion strategies to improve recall\n",
    "    - **Reciprocal Rank Fusion**: Combines rank lists by asigning high scores to documents that appear on top of any list\n",
    "    - Relative Score Fusion: normalises score within result set by defining the maximum score\n",
    "    - **Distribution Based Fusion** - uses statistical techniques such as Z-score normalization or percentile rankings to combine results making it ideal for variable scores.\n",
    "\n",
    "The Query Fusion Retriever **combines results from different retrievers** (such as vector-based and keyword-based methods) and **optionally generates multiple variations of a query using an LLM to improve coverage**. **The results are merged using fusion strategies** to improve recall.\n",
    "\n",
    "**How it works (from authoritative source)**:\n",
    "- **Combines results from multiple retrievers** - e.g., vector-based and keyword-based methods\n",
    "- **Supports multiple query variations** - generates different formulations of the same query\n",
    "- **Uses fusion strategies to improve recall** - sophisticated merging techniques\n",
    "- **Improved Coverage**: Reduces impact of query formulation on final results\n",
    "\n",
    "**Core capabilities**:\n",
    "1. **Multiple Retriever Support**: Combines results from different retrievers\n",
    "2. **Query Variation Generation**: Optionally generates multiple variations of a query using an LLM\n",
    "3. **Fusion Strategies**: Merges results using sophisticated fusion techniques\n",
    "\n",
    "**Fusion Strategies Supported (from authoritative source)**:\n",
    "1. **Reciprocal Rank Fusion (RRF)**: **Combines rankings across queries** - robust and doesn't rely on score magnitudes\n",
    "2. **Relative Score Fusion**: **Normalizes scores within each result set** - preserves the relative confidence of each retriever\n",
    "3. **Distribution-Based Fusion**: **Uses statistical normalization** - ideal for handling score variability\n",
    "\n",
    "**When to use (based on authoritative guidance):**\n",
    "- General Q&A where you want to combine semantic relevance with keyword matching\n",
    "- Complex or ambiguous queries that may benefit from multiple formulations\n",
    "- When query phrasing significantly impacts results\n",
    "- Research and exploratory search scenarios\n",
    "- When users provide under-specified or unclear queries\n",
    "\n",
    "**Configuration:**\n",
    "- `num_queries`: Number of query variations to generate (default: 4)\n",
    "- `mode`: Fusion strategy (\"reciprocal_rerank\", \"relative_score\", \"dist_based_score\")\n",
    "- `similarity_top_k`: Number of results to retrieve per query\n",
    "- `use_async`: Enable async processing for better performance\n",
    "\n",
    "**Key benefit**: **Uses fusion strategies such as reciprocal rank fusion or relative score fusion** to intelligently combine results\n",
    "\n",
    "**Strengths**: \n",
    "- Improved recall through multiple query formulations\n",
    "- Handles query variations effectively\n",
    "- Reduces query sensitivity\n",
    "- Combines strengths of different retrieval methods\n",
    "\n",
    "**Limitations**: \n",
    "- Higher computational cost due to multiple retrievers/queries\n",
    "- Requires LLM for query generation (additional cost)\n",
    "- May introduce noise if fusion strategies are not well-tuned\n",
    "- More complex setup and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231bf05-7d3f-492d-b0c6-61ea7e1348ab",
   "metadata": {},
   "source": [
    "**Reciprocal Rank Fusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e0517-535f-4403-a17b-771e915d9a08",
   "metadata": {},
   "source": [
    "Reciprocal Rank Fusion is the most robust fusion method in QueryFusionRetriever, designed to combine ranked lists from multiple query variations by using the reciprocal of ranks, which reduces the impact of outliers and provides stable fusion results.\n",
    "\n",
    "**How it works within QueryFusionRetriever**:\n",
    "- Generates multiple query variations (e.g., \"machine learning approaches\", \"ML techniques\", \"learning algorithms\")\n",
    "- Retrieves results for each query variation\n",
    "- Calculates reciprocal rank score: `1 / (rank + k)` where k is typically 60\n",
    "- Sums reciprocal rank scores across all query variations for each document\n",
    "- Re-ranks documents by combined RRF scores\n",
    "\n",
    "**Mathematical formula**:\n",
    "```\n",
    "RRF_score(d) = Σ (1 / (rank_i(d) + k))\n",
    "```\n",
    "Where:\n",
    "- `d` is a document\n",
    "- `rank_i(d)` is the rank of document d in query variation i's results\n",
    "- `k` is a constant (typically 60) that controls the fusion behavior\n",
    "\n",
    "**Why RRF works well for query fusion**:\n",
    "- **Scale-invariant**: Works regardless of individual query result score ranges\n",
    "- **Robust to outliers**: Reciprocal function reduces impact of extreme rankings\n",
    "- **Query-agnostic**: Doesn't depend on specific query formulations\n",
    "- **Proven effectiveness**: Well-established in information retrieval research\n",
    "\n",
    "**When to use RRF mode**:\n",
    "- Default choice for most query fusion scenarios\n",
    "- When query variations might have very different result qualities\n",
    "- When you want stable, predictable fusion behavior\n",
    "- For production systems requiring consistent performance\n",
    "\n",
    "**Advantages**:\n",
    "- Most stable fusion method across different query types\n",
    "- No parameter tuning required beyond the standard k=60\n",
    "- Handles varying numbers of results per query variation gracefully\n",
    "- Computationally efficient\n",
    "\n",
    "**Limitations**:\n",
    "- Loses absolute score information from individual queries\n",
    "- Treats all query variations equally (no weighting)\n",
    "- May not leverage score magnitude differences effectively\n",
    "\n",
    "*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee7b28-5d0c-4789-9f3e-0fd0ee213bf7",
   "metadata": {},
   "source": [
    "**Relative Score Fusion Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5e52e-13d4-4999-b580-4343a34d90ba",
   "metadata": {},
   "source": [
    "Relative Score Fusion normalizes retrieval scores relative to the maximum score within each query variation's results, enabling effective combination when you want to preserve score magnitude information across different query formulations.\n",
    "\n",
    "**How it works within QueryFusionRetriever**:\n",
    "- Generates multiple query variations using LLM\n",
    "- Retrieves results for each query variation\n",
    "- Normalizes each query's scores by dividing by the maximum score in that query's results\n",
    "- Creates scores in the range [0, 1] where 1 is the best result from each query variation\n",
    "- Combines normalized scores using weighted average or sum\n",
    "\n",
    "**Mathematical approach**:\n",
    "```\n",
    "normalized_score_i(d) = score_i(d) / max_score_i\n",
    "combined_score(d) = Σ (weight_i × normalized_score_i(d))\n",
    "```\n",
    "\n",
    "**Why Relative Score Fusion is valuable for query variations**:\n",
    "- **Preserves score magnitudes**: Unlike RRF, retains information about how confident each query was about its results\n",
    "- **Fair combination**: Ensures no single query variation dominates due to different scoring scales\n",
    "- **Interpretable results**: Final scores reflect the relative strength across query variations\n",
    "- **Flexible weighting**: Can weight certain query formulations more heavily if desired\n",
    "\n",
    "**When to use Relative Score mode**:\n",
    "- When you trust the embedding model's confidence scores\n",
    "- For queries where score magnitudes are meaningful\n",
    "- When different query variations should contribute proportionally to their confidence\n",
    "- In scenarios where you want to understand why certain results ranked highly\n",
    "\n",
    "**Configuration within QueryFusionRetriever**:\n",
    "- Automatically handles score normalization across query variations\n",
    "- Equal weighting of all query variations by default\n",
    "- Preserves relative differences in retriever confidence\n",
    "\n",
    "**Advantages**:\n",
    "- Preserves valuable score magnitude information\n",
    "- Intuitive normalization approach\n",
    "- Works well when retriever scores are reliable\n",
    "- More interpretable than pure rank-based methods\n",
    "\n",
    "**Limitations**:\n",
    "- Sensitive to outlier scores within individual query results\n",
    "- Assumes retriever scores are meaningful and comparable\n",
    "- May not handle unreliable scoring mechanisms well\n",
    "\n",
    "*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/relative_score_dist_fusion/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e252095-3e97-47eb-b31e-6987d15a4fa0",
   "metadata": {},
   "source": [
    "**Distribution-Based Score Fusion Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd4fb3-ca7a-4ec2-bce9-e8199291ce35",
   "metadata": {},
   "source": [
    "Distribution-Based Score Fusion uses statistical properties of score distributions from each query variation to normalize and combine retrieval results, providing the most sophisticated handling of score variability and reliability across different query formulations.\n",
    "\n",
    "**How it works within QueryFusionRetriever**:\n",
    "- Generates multiple query variations using LLM\n",
    "- Analyzes the statistical distribution of scores from each query variation\n",
    "- Normalizes scores using distribution parameters (mean, standard deviation, percentiles)\n",
    "- Applies statistical transformations like z-score normalization or percentile ranking\n",
    "- Combines normalized scores with confidence weighting based on distribution characteristics\n",
    "\n",
    "**Statistical approaches used**:\n",
    "1. **Z-score normalization**: Centers scores around mean with unit variance\n",
    "   - Formula: `z_score = (score - mean) / std_dev`\n",
    "   - Converts to [0,1] range using sigmoid: `1 / (1 + exp(-z_score))`\n",
    "\n",
    "2. **Percentile ranking**: Converts scores to percentile positions\n",
    "   - Formula: `percentile = rank(score) / total_results`\n",
    "\n",
    "3. **Distribution-aware normalization**: Considers score distribution shape\n",
    "   - Uses IQR (Interquartile Range) to adjust for distribution spread\n",
    "   - Handles multi-modal distributions from different query variations\n",
    "\n",
    "**Why Distribution-Based Fusion excels for query variations**:\n",
    "- **Statistical robustness**: Accounts for how scores are distributed within each query variation\n",
    "- **Adaptive weighting**: Can weight query variations based on their score distribution confidence\n",
    "- **Outlier handling**: Statistical methods naturally handle extreme scores\n",
    "- **Multi-modal support**: Each query variation may have different score distribution characteristics\n",
    "\n",
    "**When to use Distribution-Based mode**:\n",
    "- When query variations produce very different score distributions\n",
    "- For complex queries where some variations are much more reliable than others\n",
    "- When you need statistically principled score combination\n",
    "- In scenarios with noisy or unreliable retrieval scoring\n",
    "\n",
    "**Advanced features in QueryFusionRetriever context**:\n",
    "- Automatic distribution analysis for each query variation\n",
    "- Confidence-based weighting of query variations\n",
    "- Robust handling of varying result set sizes\n",
    "- Statistical outlier detection within query results\n",
    "\n",
    "**Advantages**:\n",
    "- Most statistically principled approach to query fusion\n",
    "- Handles complex score distributions effectively\n",
    "- Adapts to different query variation characteristics\n",
    "- Robust to various types of score variability and noise\n",
    "\n",
    "**Limitations**:\n",
    "- Most computationally intensive fusion method\n",
    "- Requires sufficient results for reliable distribution estimation\n",
    "- May over-normalize in some simple scenarios\n",
    "- More complex to interpret than simpler fusion methods\n",
    "\n",
    "*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/relative_score_dist_fusion/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a0b89-45ce-4ab3-bbb3-4606583a12aa",
   "metadata": {},
   "source": [
    "###### **Recommended retrievers by use case**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442562a5-0437-4f80-8871-ccc7dc2d59be",
   "metadata": {},
   "source": [
    "|Use Case|Primary Retriever|Secondary/Hybrid|\n",
    "|------|------|------|\n",
    "|General Q&A|Vector Index Retriever|+BM25|\n",
    "|Technical Docs|BM25 Retriever|+Vector|\n",
    "|Long Documents|Auto Merging Retriever|-|\n",
    "|Research Papers|Recursive Retriever|-|\n",
    "|Large Document Sets|Document Summary Index Retriever|+Vector|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df6111-e815-4a84-8c1b-6dfc28ed6e5d",
   "metadata": {},
   "source": [
    "Based on the authoritative source and the characteristics of each retriever, here are recommended approaches for different scenarios:\n",
    "\n",
    "**General Q&A Applications:**\n",
    "- **Primary**: Vector Index Retriever for semantic understanding\n",
    "- **Enhancement**: Combine with BM25 Retriever using Query Fusion for hybrid approach\n",
    "- **Benefit**: Combines semantic relevance with keyword matching\n",
    "- **From authoritative source**: \"For general Q&A, use a vector index retriever, potentially combined with a BM25 retriever. This retriever fusion combines semantic relevance with keyword matching.\"\n",
    "\n",
    "**Technical Documentation:**\n",
    "- **Primary**: BM25 Retriever for exact term matching\n",
    "- **Enhancement**: Vector Index Retriever as secondary for contextual flexibility\n",
    "- **Benefit**: Prioritizes exact technical terms while maintaining semantic understanding\n",
    "- **From authoritative source**: \"For technical documents, especially those where exact terms need to be prioritized, consider making BM25 your primary retriever, with the vector index retriever adding contextual flexibility as a secondary retriever.\"\n",
    "\n",
    "**Long Documents:**\n",
    "- **Primary**: Auto Merging Retriever\n",
    "- **Benefit**: Retrieves longer parent versions only if enough shorter child versions are retrieved, preserving context\n",
    "- **From authoritative source**: \"For long documents, the auto merging retriever is a great option, because it will retrieve longer parent versions only if enough shorter child versions are retrieved.\"\n",
    "\n",
    "**Research Papers:**\n",
    "- **Primary**: Recursive Retriever\n",
    "- **Benefit**: Follows citations and references to retrieve relevant content from cited papers\n",
    "- **From authoritative source**: \"For research papers, use the recursive retriever in order to retrieve relevant content from cited papers.\"\n",
    "\n",
    "**Large Document Collections:**\n",
    "- **Primary**: Document Summary Index Retriever for initial filtering\n",
    "- **Enhancement**: Followed by Vector Index Retriever for detailed search within relevant documents\n",
    "- **Benefit**: Narrows down relevant documents first, then performs detailed retrieval\n",
    "- **From authoritative source**: \"For large document sets, consider using the document summary index retriever to narrow down the number of relevant documents, followed by a vector search within the remaining subset to retrieve the most pertinent content.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec127ad-fc1c-407c-8b5a-824a48c72bee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lab 2 - Explore Advanced Retrievers in Llama Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a1861-8c18-4bb5-99a7-83c1e2f4fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Below will fail unless the appropriate Python version is running\n",
    "!{sys.executable} -m pip install llama-index==0.12.49 \\\n",
    "    llama-index-embeddings-huggingface==0.5.5 \\\n",
    "    llama-index-llms-ibm==0.4.0 \\\n",
    "    llama-index-retrievers-bm25==0.5.2 \\\n",
    "    sentence-transformers==5.0.0 \\\n",
    "    rank-bm25==0.2.2 \\\n",
    "    PyStemmer==2.2.0.3 \\\n",
    "    ibm-watsonx-ai==1.3.31 | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb804ad-6232-4564-9ff0-d0f28ce7726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    Document,\n",
    "    Settings,\n",
    "    DocumentSummaryIndex,\n",
    "    KeywordTableIndex\n",
    ")\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    AutoMergingRetriever,\n",
    "    RecursiveRetriever,\n",
    "    QueryFusionRetriever\n",
    ")\n",
    "from llama_index.core.indices.document_summary import (\n",
    "    DocumentSummaryIndexLLMRetriever,\n",
    "    DocumentSummaryIndexEmbeddingRetriever,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter, HierarchicalNodeParser\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Advanced retriever imports\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# IBM WatsonX LlamaIndex integration\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "\n",
    "# Sentence transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Statistical libraries for fusion techniques\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    print(\"⚠️ scipy not available - some advanced fusion features will be limited\")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8430c76-c37a-47f4-9cc3-668123247226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watsonx.ai LLM using official LlamaIndex integration\n",
    "def create_watsonx_llm():\n",
    "    \"\"\"Create watsonx.ai LLM instance using official LlamaIndex integration.\"\"\"\n",
    "    try:\n",
    "        # Create the API client object\n",
    "        api_client = APIClient({'url': \"https://us-south.ml.cloud.ibm.com\"})\n",
    "        # Use llama-index-llms-ibm (official watsonx.ai integration)\n",
    "        llm = WatsonxLLM(\n",
    "            model_id=\"ibm/granite-3-3-8b-instruct\",\n",
    "            url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "            project_id=\"skills-network\",\n",
    "            api_client=api_client,\n",
    "            temperature=0.9\n",
    "        )\n",
    "        print(\"✅ watsonx.ai LLM initialized using official LlamaIndex integration\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ watsonx.ai initialization error: {e}\")\n",
    "        print(\"Falling back to mock LLM for demonstration\")\n",
    "        \n",
    "        # Fallback mock LLM for demonstration\n",
    "        from llama_index.core.llms.mock import MockLLM\n",
    "        return MockLLM(max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73697282-82de-4f3b-b7ae-f29e92c4c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model first\n",
    "print(\"🔧 Initializing HuggingFace embeddings...\")\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "print(\"✅ HuggingFace embeddings initialized!\")\n",
    "\n",
    "# Setup with watsonx.ai\n",
    "print(\"🔧 Initializing watsonx.ai LLM...\")\n",
    "llm = create_watsonx_llm()\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "print(\"✅ watsonx.ai LLM and embeddings configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dea033-7671-4004-bce5-5b7b9ab86707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for the lab - AI/ML focused documents\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "    \"Natural language processing enables computers to understand, interpret, and generate human language.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world.\",\n",
    "    \"Reinforcement learning is a type of machine learning where agents learn to make decisions through rewards and penalties.\",\n",
    "    \"Supervised learning uses labeled training data to learn a mapping from inputs to outputs.\",\n",
    "    \"Unsupervised learning finds hidden patterns in data without labeled examples.\",\n",
    "    \"Transfer learning leverages knowledge from pre-trained models to improve performance on new tasks.\",\n",
    "    \"Generative AI can create new content including text, images, code, and more.\",\n",
    "    \"Large language models are trained on vast amounts of text data to understand and generate human-like text.\"\n",
    "]\n",
    "\n",
    "# Consistent query examples used throughout the lab\n",
    "DEMO_QUERIES = {\n",
    "    \"basic\": \"What is machine learning?\",\n",
    "    \"technical\": \"neural networks deep learning\", \n",
    "    \"learning_types\": \"different types of learning\",\n",
    "    \"advanced\": \"How do neural networks work in deep learning?\",\n",
    "    \"applications\": \"What are the applications of AI?\",\n",
    "    \"comprehensive\": \"What are the main approaches to machine learning?\",\n",
    "    \"specific\": \"supervised learning techniques\"\n",
    "}\n",
    "\n",
    "print(f\"📄 Loaded {len(SAMPLE_DOCUMENTS)} sample documents\")\n",
    "print(f\"🔍 Prepared {len(DEMO_QUERIES)} consistent demo queries\")\n",
    "for i, doc in enumerate(SAMPLE_DOCUMENTS[:3], 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c405f5f-acb8-443b-aac8-58587e210a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRetrieversLab:\n",
    "    def __init__(self):\n",
    "        print(\"🚀 Initializing Advanced Retrievers Lab...\")\n",
    "        self.documents = [Document(text=text) for text in SAMPLE_DOCUMENTS]\n",
    "        self.nodes = SentenceSplitter().get_nodes_from_documents(self.documents)\n",
    "        \n",
    "        print(\"📊 Creating indexes...\")\n",
    "        # Create various indexes\n",
    "        self.vector_index = VectorStoreIndex.from_documents(self.documents)\n",
    "        self.document_summary_index = DocumentSummaryIndex.from_documents(self.documents)\n",
    "        self.keyword_index = KeywordTableIndex.from_documents(self.documents)\n",
    "        \n",
    "        print(\"✅ Advanced Retrievers Lab Initialized!\")\n",
    "        print(f\"📄 Loaded {len(self.documents)} documents\")\n",
    "        print(f\"🔢 Created {len(self.nodes)} nodes\")\n",
    "\n",
    "# Initialize the lab\n",
    "lab = AdvancedRetrieversLab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd729f-df6c-4335-8d79-27ee35ce84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"1. VECTOR INDEX RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic vector retriever\n",
    "vector_retriever = VectorIndexRetriever(\n",
    "    index=lab.vector_index,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# Alternative creation method\n",
    "alt_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query = DEMO_QUERIES[\"basic\"]  # \"What is machine learning?\"\n",
    "nodes = vector_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(nodes)} nodes:\")\n",
    "for i, node in enumerate(nodes, 1):\n",
    "    print(f\"{i}. Score: {node.score:.4f}\")\n",
    "    print(f\"   Text: {node.text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829714bc-93a2-4f01-9fd6-f65676f706ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"2. BM25 RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import Stemmer\n",
    "    \n",
    "    # Create BM25 retriever with default parameters\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=lab.nodes,\n",
    "        similarity_top_k=3,\n",
    "        stemmer=Stemmer.Stemmer(\"english\"),\n",
    "        language=\"english\"\n",
    "    )\n",
    "    \n",
    "    query = DEMO_QUERIES[\"technical\"]  # \"neural networks deep learning\"\n",
    "    nodes = bm25_retriever.retrieve(query)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"BM25 analyzes exact keyword matches with sophisticated scoring\")\n",
    "    print(f\"Retrieved {len(nodes)} nodes:\")\n",
    "    \n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        score = node.score if hasattr(node, 'score') and node.score else 0\n",
    "        print(f\"{i}. BM25 Score: {score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        \n",
    "        # Highlight which query terms appear in the text\n",
    "        text_lower = node.text.lower()\n",
    "        query_terms = query.lower().split()\n",
    "        found_terms = [term for term in query_terms if term in text_lower]\n",
    "        if found_terms:\n",
    "            print(f\"   → Found terms: {found_terms}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"BM25 vs TF-IDF Comparison:\")\n",
    "    print(\"TF-IDF Problem: Linear term frequency scaling\")\n",
    "    print(\"  Example: 10 occurrences → score of 10, 100 occurrences → score of 100\")\n",
    "    print(\"BM25 Solution: Saturation function\")\n",
    "    print(\"  Example: 10 occurrences → high score, 100 occurrences → slightly higher score\")\n",
    "    print()\n",
    "    print(\"TF-IDF Problem: No document length consideration\")\n",
    "    print(\"  Example: Long documents dominate results\")\n",
    "    print(\"BM25 Solution: Length normalization (b parameter)\")\n",
    "    print(\"  Example: Scores adjusted based on document length vs. average\")\n",
    "    print()\n",
    "    print(\"Key BM25 Parameters:\")\n",
    "    print(\"- k1 ≈ 1.2: Term frequency saturation (how quickly scores plateau)\")\n",
    "    print(\"- b ≈ 0.75: Document length normalization (0=none, 1=full)\")\n",
    "    print(\"- IDF weighting: Rare terms get higher scores\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"⚠️ BM25Retriever requires 'pip install PyStemmer'\")\n",
    "    print(\"Demonstrating BM25 concepts with fallback vector search...\")\n",
    "    \n",
    "    fallback_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "    query = DEMO_QUERIES[\"technical\"]\n",
    "    nodes = fallback_retriever.retrieve(query)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"(Using vector fallback to demonstrate BM25 concepts)\")\n",
    "    \n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Vector Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        \n",
    "        # Demonstrate TF-IDF concept manually\n",
    "        text_lower = node.text.lower()\n",
    "        query_terms = query.lower().split()\n",
    "        found_terms = [term for term in query_terms if term in text_lower]\n",
    "        \n",
    "        if found_terms:\n",
    "            print(f\"   → BM25 would boost this result for terms: {found_terms}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"BM25 Concept Demonstration:\")\n",
    "    print(\"1. TF-IDF Foundation:\")\n",
    "    print(\"   - Term Frequency: How often words appear in document\")\n",
    "    print(\"   - Inverse Document Frequency: How rare words are across collection\")\n",
    "    print(\"   - TF-IDF = TF × IDF (balances frequency vs rarity)\")\n",
    "    print()\n",
    "    print(\"2. BM25 Improvements:\")\n",
    "    print(\"   - Saturation: Prevents over-scoring repeated terms\")\n",
    "    print(\"   - Length normalization: Prevents long document bias\")\n",
    "    print(\"   - Tunable parameters: k1 (saturation) and b (length adjustment)\")\n",
    "    print()\n",
    "    print(\"3. Real-world Usage:\")\n",
    "    print(\"   - Elasticsearch default scoring function\")\n",
    "    print(\"   - Apache Lucene/Solr standard\")\n",
    "    print(\"   - Used in 83% of text-based recommender systems\")\n",
    "    print(\"   - Developed by Robertson & Spärck Jones at City University London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ef6b2-a443-4228-b786-86ce89e75373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"3. DOCUMENT SUMMARY INDEX RETRIEVERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LLM-based document summary retriever\n",
    "doc_summary_retriever_llm = DocumentSummaryIndexLLMRetriever(\n",
    "    lab.document_summary_index,\n",
    "    choice_top_k=3  # Number of documents to select\n",
    ")\n",
    "\n",
    "# Embedding-based document summary retriever  \n",
    "doc_summary_retriever_embedding = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    lab.document_summary_index,\n",
    "    similarity_top_k=3  # Number of documents to select\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"learning_types\"]  # \"different types of learning\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "print(\"\\nA) LLM-based Document Summary Retriever:\")\n",
    "print(\"Uses LLM to select relevant documents based on summaries\")\n",
    "try:\n",
    "    nodes_llm = doc_summary_retriever_llm.retrieve(query)\n",
    "    print(f\"Retrieved {len(nodes_llm)} nodes\")\n",
    "    for i, node in enumerate(nodes_llm[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
    "        print(f\"   Text: {node.text[:80]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"LLM-based retrieval demo: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"B) Embedding-based Document Summary Retriever:\")\n",
    "print(\"Uses vector similarity between query and document summaries\")\n",
    "try:\n",
    "    nodes_emb = doc_summary_retriever_embedding.retrieve(query)\n",
    "    print(f\"Retrieved {len(nodes_emb)} nodes\")\n",
    "    for i, node in enumerate(nodes_emb[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
    "        print(f\"   Text: {node.text[:80]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"Embedding-based retrieval demo: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"Document Summary Index workflow:\")\n",
    "print(\"1. Generates summaries for each document using LLM\")\n",
    "print(\"2. Uses summaries to select relevant documents\")\n",
    "print(\"3. Returns full content from selected documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0fdfc-37b5-4543-9466-5606e9079136",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4. AUTO MERGING RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create hierarchical nodes\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[512, 256, 128]\n",
    ")\n",
    "\n",
    "hier_nodes = node_parser.get_nodes_from_documents(lab.documents)\n",
    "\n",
    "# Create storage context with all nodes\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(hier_nodes)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "# Create base index\n",
    "base_index = VectorStoreIndex(hier_nodes, storage_context=storage_context)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "\n",
    "# Create auto-merging retriever\n",
    "auto_merging_retriever = AutoMergingRetriever(\n",
    "    base_retriever, \n",
    "    storage_context,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"advanced\"]  # \"How do neural networks work in deep learning?\"\n",
    "nodes = auto_merging_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Auto-merged to {len(nodes)} nodes\")\n",
    "for i, node in enumerate(nodes[:3], 1):\n",
    "    print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Auto-merged)\")\n",
    "    print(f\"   Text: {node.text[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278e49e-b60e-4372-a812-81074ed2ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.1 RECIPROCAL RANK FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create QueryFusionRetriever with RRF mode\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with reciprocal_rerank mode:\")\n",
    "print(\"This demonstrates how RRF works within the query fusion framework\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with RRF mode\n",
    "    rrf_query_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"reciprocal_rerank\",\n",
    "        use_async=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever will:\")\n",
    "    print(\"1. Generate query variations using LLM\")\n",
    "    print(\"2. Retrieve results for each variation\")\n",
    "    print(\"3. Apply Reciprocal Rank Fusion\")\n",
    "    \n",
    "    nodes = rrf_query_fusion.retrieve(query)\n",
    "    \n",
    "    print(f\"\\nRRF Query Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Final RRF Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"RRF Benefits in Query Fusion Context:\")\n",
    "    print(\"- Automatically handles query variations of different quality\")\n",
    "    print(\"- No bias toward queries that return higher raw scores\")\n",
    "    print(\"- Stable performance across diverse query formulations\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating RRF concept manually with query variations...\")\n",
    "    \n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Manual RRF with Query Variations:\")\n",
    "    all_results = {}\n",
    "    \n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        \n",
    "        # Apply RRF scoring\n",
    "        for rank, node in enumerate(nodes):\n",
    "            node_id = node.node.node_id\n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'rrf_score': 0,\n",
    "                    'query_ranks': []\n",
    "                }\n",
    "            \n",
    "            # Calculate RRF contribution: 1 / (rank + k)\n",
    "            k = 60  # Standard RRF parameter\n",
    "            rrf_contribution = 1.0 / (rank + 1 + k)\n",
    "            all_results[node_id]['rrf_score'] += rrf_contribution\n",
    "            all_results[node_id]['query_ranks'].append((i, rank + 1))\n",
    "    \n",
    "    # Sort by final RRF score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(), \n",
    "        key=lambda x: x['rrf_score'], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined RRF Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Final RRF Score: {result['rrf_score']:.4f}\")\n",
    "        print(f\"   Query ranks: {result['query_ranks']}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"RRF Formula Demonstration:\")\n",
    "    print(\"For each document: RRF_score = Σ(1 / (rank + 60))\")\n",
    "    print(\"- Rank 1 in query: 1/(1+60) = 0.0164\")\n",
    "    print(\"- Rank 2 in query: 1/(2+60) = 0.0161\")\n",
    "    print(\"- Rank 3 in query: 1/(3+60) = 0.0159\")\n",
    "    print(\"Documents appearing in multiple queries get higher combined scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf4f2e-8215-4b4e-8b13-e160e02c03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.2 RELATIVE SCORE FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with relative_score mode:\")\n",
    "print(\"This mode preserves score magnitudes while normalizing across query variations\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with relative score mode\n",
    "    rel_score_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"relative_score\",\n",
    "        use_async=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever with relative_score will:\")\n",
    "    print(\"1. Generate query variations\")\n",
    "    print(\"2. Normalize scores within each variation (score/max_score)\")\n",
    "    print(\"3. Combine normalized scores\")\n",
    "    \n",
    "    nodes = rel_score_fusion.retrieve(query)\n",
    "    \n",
    "    print(f\"\\nRelative Score Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Combined Relative Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Relative Score Benefits in Query Fusion:\")\n",
    "    print(\"- Preserves confidence information from embedding model\")\n",
    "    print(\"- Ensures fair contribution from each query variation\")\n",
    "    print(\"- More interpretable than rank-only methods\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating Relative Score concept manually...\")\n",
    "    \n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Manual Relative Score Fusion with Query Variations:\")\n",
    "    all_results = {}\n",
    "    query_max_scores = []\n",
    "    \n",
    "    # Step 1: Get results and find max scores for each query\n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        scores = [node.score or 0 for node in nodes]\n",
    "        max_score = max(scores) if scores else 1.0\n",
    "        query_max_scores.append(max_score)\n",
    "        \n",
    "        print(f\"Max score for this query: {max_score:.4f}\")\n",
    "        \n",
    "        # Store results with normalization info\n",
    "        for node in nodes:\n",
    "            node_id = node.node.node_id\n",
    "            original_score = node.score or 0\n",
    "            normalized_score = original_score / max_score if max_score > 0 else 0\n",
    "            \n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'combined_score': 0,\n",
    "                    'contributions': []\n",
    "                }\n",
    "            \n",
    "            all_results[node_id]['combined_score'] += normalized_score\n",
    "            all_results[node_id]['contributions'].append({\n",
    "                'query': i,\n",
    "                'original': original_score,\n",
    "                'normalized': normalized_score\n",
    "            })\n",
    "    \n",
    "    # Step 2: Sort by combined relative score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(),\n",
    "        key=lambda x: x['combined_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined Relative Score Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Score breakdown:\")\n",
    "        for contrib in result['contributions']:\n",
    "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} → {contrib['normalized']:.3f}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Relative Score Normalization Process:\")\n",
    "    print(\"1. For each query variation, find max_score\")\n",
    "    print(\"2. Normalize: normalized_score = original_score / max_score\")\n",
    "    print(\"3. Sum normalized scores across all query variations\")\n",
    "    print(\"4. Documents with consistently high scores across queries win\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24188d65-6506-4180-bbda-f5ee69e6de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.3 DISTRIBUTION-BASED SCORE FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=8)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with dist_based_score mode:\")\n",
    "print(\"This mode uses statistical analysis for the most sophisticated score fusion\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with distribution-based mode\n",
    "    dist_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"dist_based_score\",\n",
    "        use_async=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever with dist_based_score will:\")\n",
    "    print(\"1. Generate query variations\")\n",
    "    print(\"2. Analyze score distributions for each variation\")\n",
    "    print(\"3. Apply statistical normalization (z-score, percentiles)\")\n",
    "    print(\"4. Combine with distribution-aware weighting\")\n",
    "    \n",
    "    nodes = dist_fusion.retrieve(query)\n",
    "    \n",
    "    print(f\"\\nDistribution-Based Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Statistically Normalized Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Distribution-Based Benefits in Query Fusion:\")\n",
    "    print(\"- Accounts for score distribution differences between query variations\")\n",
    "    print(\"- Statistically robust against outliers and noise\")\n",
    "    print(\"- Adapts weighting based on query variation reliability\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating Distribution-Based concept manually...\")\n",
    "    \n",
    "    if not SCIPY_AVAILABLE:\n",
    "        print(\"⚠️ Full statistical analysis requires scipy\")\n",
    "    \n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Manual Distribution-Based Fusion with Query Variations:\")\n",
    "    all_results = {}\n",
    "    variation_stats = []\n",
    "    \n",
    "    # Step 1: Collect results and analyze distributions\n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        scores = [node.score or 0 for node in nodes]\n",
    "        \n",
    "        # Calculate distribution statistics\n",
    "        mean_score = np.mean(scores) if scores else 0\n",
    "        std_score = np.std(scores) if len(scores) > 1 else 1\n",
    "        min_score = np.min(scores) if scores else 0\n",
    "        max_score = np.max(scores) if scores else 1\n",
    "        \n",
    "        stats_info = {\n",
    "            'mean': mean_score,\n",
    "            'std': std_score,\n",
    "            'min': min_score,\n",
    "            'max': max_score,\n",
    "            'nodes': nodes,\n",
    "            'scores': scores\n",
    "        }\n",
    "        variation_stats.append(stats_info)\n",
    "        \n",
    "        print(f\"Distribution stats: mean={mean_score:.3f}, std={std_score:.3f}\")\n",
    "        print(f\"Score range: [{min_score:.3f}, {max_score:.3f}]\")\n",
    "        \n",
    "        # Apply z-score normalization\n",
    "        for node, score in zip(nodes, scores):\n",
    "            node_id = node.node.node_id\n",
    "            \n",
    "            # Z-score normalization\n",
    "            if std_score > 0:\n",
    "                z_score = (score - mean_score) / std_score\n",
    "            else:\n",
    "                z_score = 0\n",
    "            \n",
    "            # Convert to [0,1] using sigmoid\n",
    "            normalized_score = 1 / (1 + np.exp(-z_score))\n",
    "            \n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'combined_score': 0,\n",
    "                    'contributions': []\n",
    "                }\n",
    "            \n",
    "            all_results[node_id]['combined_score'] += normalized_score\n",
    "            all_results[node_id]['contributions'].append({\n",
    "                'query': i,\n",
    "                'original': score,\n",
    "                'z_score': z_score,\n",
    "                'normalized': normalized_score\n",
    "            })\n",
    "    \n",
    "    # Step 2: Sort by combined distribution-based score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(),\n",
    "        key=lambda x: x['combined_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined Distribution-Based Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Statistical breakdown:\")\n",
    "        for contrib in result['contributions']:\n",
    "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} → \"\n",
    "                  f\"z={contrib['z_score']:.2f} → {contrib['normalized']:.3f}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Distribution-Based Process:\")\n",
    "    print(\"1. Calculate mean and std for each query variation\")\n",
    "    print(\"2. Z-score normalize: z = (score - mean) / std\")\n",
    "    print(\"3. Sigmoid transform: normalized = 1 / (1 + exp(-z))\")\n",
    "    print(\"4. Sum normalized scores across variations\")\n",
    "    print(\"5. Results reflect statistical significance across all query forms\")\n",
    "\n",
    "# Show fusion mode comparison summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FUSION MODES COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"All three modes tested with the same query for direct comparison:\")\n",
    "print(f\"Query: {query}\")\n",
    "print()\n",
    "print(\"Mode Characteristics:\")\n",
    "print(\"• RRF (reciprocal_rerank): Most robust, rank-based, scale-invariant\")\n",
    "print(\"• Relative Score: Preserves confidence, normalizes by max score\")  \n",
    "print(\"• Distribution-Based: Most sophisticated, statistical normalization\")\n",
    "print()\n",
    "print(\"Choose based on your use case:\")\n",
    "print(\"- Production stability → RRF\")\n",
    "print(\"- Score interpretability → Relative Score\")\n",
    "print(\"- Statistical robustness → Distribution-Based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aa9ba-7033-4afd-bdf9-410dde9024a9",
   "metadata": {},
   "source": [
    "##### Exercise 1 - Build a Custom Hybrid Retriever\n",
    "\n",
    "Your task is to create a hybrid retriever that combines both vector similarity and BM25 keyword search for improved results.\n",
    "\n",
    "**Requirements:**\n",
    "- Use both Vector Index Retriever and BM25 Retriever\n",
    "- Implement a simple score fusion mechanism which takes a weighted average of normalized scores\n",
    "- Test with different query types (semantic vs keyword-focused)\n",
    "\n",
    "**Important Note**: Node IDs from different retrievers won't match even for the same content, so we need to match by text content instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87dff4c-cc56-47c0-a406-56ad66928fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both retrievers\n",
    "vector_retriever = lab.vector_index.as_retriever(similarity_top_k=10)\n",
    "try:\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=lab.nodes, similarity_top_k=10\n",
    "    )\n",
    "except:\n",
    "    # Fallback if BM25 is not available\n",
    "    bm25_retriever = vector_retriever\n",
    "\n",
    "def hybrid_retrieve(query, top_k=5):\n",
    "    # Get results from both retrievers\n",
    "    vector_results = vector_retriever.retrieve(query)\n",
    "    bm25_results = bm25_retriever.retrieve(query)\n",
    "    \n",
    "    # Create dictionaries using text content as keys (since node IDs differ)\n",
    "    vector_scores = {}\n",
    "    bm25_scores = {}\n",
    "    all_nodes = {}\n",
    "    \n",
    "    # Normalize vector scores\n",
    "    max_vector_score = max([r.score for r in vector_results]) if vector_results else 1\n",
    "    for result in vector_results:\n",
    "        text_key = result.text.strip()  # Use text content as key\n",
    "        normalized_score = result.score / max_vector_score\n",
    "        vector_scores[text_key] = normalized_score\n",
    "        all_nodes[text_key] = result\n",
    "    \n",
    "    # Normalize BM25 scores\n",
    "    max_bm25_score = max([r.score for r in bm25_results]) if bm25_results else 1\n",
    "    for result in bm25_results:\n",
    "        text_key = result.text.strip()  # Use text content as key\n",
    "        normalized_score = result.score / max_bm25_score\n",
    "        bm25_scores[text_key] = normalized_score\n",
    "        all_nodes[text_key] = result\n",
    "    \n",
    "    # Calculate hybrid scores\n",
    "    hybrid_results = []\n",
    "    for text_key in all_nodes:\n",
    "        vector_score = vector_scores.get(text_key, 0)\n",
    "        bm25_score = bm25_scores.get(text_key, 0)\n",
    "        hybrid_score = 0.7 * vector_score + 0.3 * bm25_score\n",
    "        \n",
    "        hybrid_results.append({\n",
    "            'node': all_nodes[text_key],\n",
    "            'vector_score': vector_score,\n",
    "            'bm25_score': bm25_score,\n",
    "            'hybrid_score': hybrid_score\n",
    "        })\n",
    "    \n",
    "    # Sort by hybrid score and return top k\n",
    "    hybrid_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "    return hybrid_results[:top_k]\n",
    "\n",
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"neural networks deep learning\", \n",
    "    \"supervised learning techniques\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    results = hybrid_retrieve(query, top_k=3)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. Hybrid Score: {result['hybrid_score']:.3f}\")\n",
    "        print(f\"   Vector: {result['vector_score']:.3f}, BM25: {result['bm25_score']:.3f}\")\n",
    "        print(f\"   Text: {result['node'].text[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002ec35-37d9-4621-b2ea-0db2b5e137be",
   "metadata": {},
   "source": [
    "##### Exercise 2 - Create a Production RAG Pipeline\n",
    "\n",
    "Build a complete RAG pipeline that uses multiple retrieval strategies and includes evaluation metrics.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement retrieval with multiple strategies\n",
    "- Add query routing logic\n",
    "- Include basic evaluation metrics that evaluate whether the pipeline succeeded or failed\n",
    "- Handle edge cases and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8c7f7-f276-4069-b30b-243737e3ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionRAGPipeline:\n",
    "    def __init__(self, index, llm):\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "        self.vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "        \n",
    "    def _route_query(self, question):\n",
    "        \"\"\"Simple query routing based on question characteristics\"\"\"\n",
    "        if any(word in question.lower() for word in [\"what\", \"explain\", \"describe\"]):\n",
    "            return \"semantic\"\n",
    "        elif any(word in question.lower() for word in [\"list\", \"types\", \"examples\"]):\n",
    "            return \"comprehensive\"\n",
    "        else:\n",
    "            return \"semantic\"\n",
    "    \n",
    "    def query(self, question, strategy=\"auto\"):\n",
    "        try:\n",
    "            # Route query if strategy is auto\n",
    "            if strategy == \"auto\":\n",
    "                strategy = self._route_query(question)\n",
    "            \n",
    "            # Retrieve relevant documents\n",
    "            if strategy == \"semantic\":\n",
    "                retriever = self.vector_retriever\n",
    "                top_k = 3\n",
    "            elif strategy == \"comprehensive\":\n",
    "                retriever = self.vector_retriever\n",
    "                top_k = 5\n",
    "            else:\n",
    "                retriever = self.vector_retriever\n",
    "                top_k = 3\n",
    "            \n",
    "            # Get relevant documents\n",
    "            relevant_docs = retriever.retrieve(question)\n",
    "            \n",
    "            # Prepare context\n",
    "            context = \"\\n\\n\".join([doc.text for doc in relevant_docs[:top_k]])\n",
    "            \n",
    "            # Generate response\n",
    "            prompt = f\"\"\"Based on the following context, please answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = self.llm.complete(prompt)\n",
    "                return {\n",
    "                    \"answer\": response.text,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"num_docs\": len(relevant_docs),\n",
    "                    \"status\": \"success\"\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"answer\": f\"Based on the retrieved documents: {context[:200]}...\",\n",
    "                    \"strategy\": strategy,\n",
    "                    \"num_docs\": len(relevant_docs),\n",
    "                    \"status\": f\"llm_error: {str(e)}\"\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": \"I encountered an error processing your question.\",\n",
    "                \"strategy\": strategy,\n",
    "                \"num_docs\": 0,\n",
    "                \"status\": f\"error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    def evaluate(self, test_queries):\n",
    "        results = []\n",
    "        for query in test_queries:\n",
    "            result = self.query(query)\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"result\": result,\n",
    "                \"success\": result[\"status\"] == \"success\"\n",
    "            })\n",
    "        \n",
    "        success_rate = sum(1 for r in results if r[\"success\"]) / len(results)\n",
    "        return {\n",
    "            \"success_rate\": success_rate,\n",
    "            \"results\": results\n",
    "        }\n",
    "\n",
    "# Test the pipeline\n",
    "pipeline = ProductionRAGPipeline(lab.vector_index, llm)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"List different types of learning algorithms\",\n",
    "    \"Explain neural networks\"\n",
    "]\n",
    "\n",
    "print(\"Testing Production RAG Pipeline:\")\n",
    "for query in test_queries:\n",
    "    result = pipeline.query(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Strategy: {result['strategy']}\")\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    print(f\"Answer: {result['answer'][:100]}...\")\n",
    "\n",
    "# Evaluate performance\n",
    "evaluation = pipeline.evaluate(test_queries)\n",
    "print(f\"\\nPipeline Success Rate: {evaluation['success_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9629a0-5cc9-43ba-ac1f-9076f4414b43",
   "metadata": {},
   "source": [
    "### Module 2 - Build a Comprehensive RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248899b-f2af-4e8d-98c1-728639c09203",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lecture 1 - Introduction to FAISS and how it compares to ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833096b0-6389-4d87-862e-7815abdce722",
   "metadata": {},
   "source": [
    "**Facebook AI Similarity Search** or **FAISS**\n",
    "- is a library created by meta for fast vector search\n",
    "- runs on a single machine, either CPU or GPU\n",
    "- you write code to use it, as it has no database storage or server\n",
    "- is great for custom, high-performance systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d613e1-20df-4145-aaec-80eb2dc8f33b",
   "metadata": {},
   "source": [
    "**ChromaDB**\n",
    "- is a vector DB designed for AI applications\n",
    "- stores vector and metadata together\n",
    "- can run locally or as a server\n",
    "- easy to use with tools like LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f72c41-befd-4412-9067-394a7b810bf1",
   "metadata": {},
   "source": [
    "|Features|FAISS|ChromaDB|\n",
    "|------|------|------|\n",
    "|Type|Library|Database|\n",
    "|Deployment|Local, single node|Local, single node or distributed|\n",
    "|Indexing Options|Flat, IVF, LSH, HNSW|HNSW only|\n",
    "|Metadata support|None|Storage, filtering|\n",
    "|LangChain/LlamaIndex support|Yes|Yes|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c3d22-77fe-4968-9101-98326166672d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Indexing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923c41a-1ae3-412a-804b-140f014bce4c",
   "metadata": {},
   "source": [
    "**Flat Indexing**\n",
    "\n",
    "<img src=\"images/FAISS-flat-indexing.png\" width=400/>\n",
    "\n",
    "- Stores embeddings of all documents\n",
    "- embeds the query\n",
    "- measures distance of query embedding from all vectors using dot product or euclidean distance in a **brute force** way\n",
    "- then returns the k-nearest vectors ordered from closest to farthest\n",
    "\n",
    "Very accurate, but very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692aaeb-e6d6-48a9-9d5e-8dd2e37ea06d",
   "metadata": {},
   "source": [
    "**Inverted File Index (IVF)**\n",
    "\n",
    "<img src=\"images/FAISS-IVF-Index.png\" width=400/>\n",
    "\n",
    "- Clusters vectors using techniques like k-means\n",
    "- forms Voronoi cells around centroids\n",
    "- each cell contains vectors closest to its centroid\n",
    "- when a query vector is introduced, the search is limited to vectors in the nearest cell\n",
    "\n",
    "it is faster than a flat-index, but may slightly reduce accuracy as some nearby vectors may lie in another cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810c240-a37a-4b63-8a3f-dd18839c62e7",
   "metadata": {},
   "source": [
    "**Locality-Sensitive Hashing (LSH)**\n",
    "- uses hash functions to group similar vectors\n",
    "- best for high dimensional sparse data\n",
    "- not the fastest or most accurate method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca789f-15a4-48c5-991b-624c92ee6e62",
   "metadata": {},
   "source": [
    "**Hierarchical Navigable Small World (HNSW)**\n",
    "\n",
    "<img src=\"images/FAISS-HNSW-Index.png\" width=400/>\n",
    "\n",
    "- top layer serves like an express highway to quickly relocate to the right \"region\"\n",
    "- lower layers then help reach the final nearest vectors\n",
    "- fast and accurate, especially for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9012f6-1734-4d07-9528-4e4393206fe6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Extending FAISS with Milvus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e39c90-6c09-4fdf-97e4-6d582a301716",
   "metadata": {},
   "source": [
    "- FAISS offers high performance vector search but lacks certain features like metadata support and distributed scaling\n",
    "- Milvus build on FAISS to add missing production ready capabilities\n",
    "- Adds metadata storage and filtering for hybrid query capabilities\n",
    "- Supports distributed and scalable deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f3f5e-849e-4ebf-b3b5-f67bf0d05e94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **FAISS, Milvus or Chroma DB - when to use which?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8105562-6025-47a0-9759-b6916e69c900",
   "metadata": {},
   "source": [
    "|FAISS|Chroma DB|Milvus|\n",
    "|------|------|------|\n",
    "|Custome High Performance Systems|Fast Prototyping|Distributed production-scale systems|\n",
    "|GPU Acceleration|Metadata-rich queries|FAISS-style indexing with database features|\n",
    "|Local-only deployments||Hybrid search (vector+metadata)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be736c-0c41-4c6c-ab1a-333b6785cdd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Lab 1 - Semantic Similarity with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c1d6e-61a3-4bb6-9dd0-d010185a4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install faiss-cpu numpy scikit-learn\n",
    "!{sys.executable} -m pip install \"tensorflow>=2.0.0\"\n",
    "!{sys.executable} -m pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01475c5f-f6ee-4ffc-9a7f-73051a847825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# Suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe3c3e-c39e-49d3-a91b-7e865f2077b3",
   "metadata": {},
   "source": [
    "**The 20 Newsgroups Dataset**\n",
    "\n",
    "In this project, we'll be using the 20 Newsgroups dataset, a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. It's a go-to dataset in the NLP community because it presents real-world challenges:\n",
    "\n",
    "**What is the 20 Newsgroups Dataset?**\n",
    "\n",
    "- **Diverse Topics**: The dataset spans 20 different topics, from sports and science to politics and religion, reflecting the diverse interests of newsgroup members.\n",
    "- **Natural Language**: It contains actual discussions, with all the nuances of human language, making it ideal for semantic search.\n",
    "- **Prevalence of Context**: The conversations within it require understanding of context to differentiate between the topics effectively.\n",
    "\n",
    "**How are we using the 20 Newsgroups Dataset?**\n",
    "\n",
    "1. **Exploring Data**: We'll start by loading the dataset and exploring its structure to understand the kind of information it holds.\n",
    "2. **Preprocessing**: We'll clean the text data, removing any unwanted noise that could affect our semantic analysis.\n",
    "3. **Vectorization**: We'll then use the Universal Sentence Encoder to transform this text into numerical vectors that capture the essence of each document.\n",
    "4. **Semantic Search Implementation**: Finally, we'll use FAISS to index these vectors, allowing us to perform fast and efficient semantic searches across the dataset.\n",
    "\n",
    "By working with the 20 Newsgroups dataset, you'll gain hands-on experience with real-world data and the end-to-end process of building a semantic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d16a18e9-3a06-43e3-90f1-d9ade60c990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb9cac-8ea8-4890-a17d-93eeb09c1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca0fdf-e6f8-4ed5-bad3-370f2aa74b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 3 posts from the dataset\n",
    "for i in range(3):\n",
    "    print(f\"Sample post {i+1}:\\n\")\n",
    "    pprint(newsgroups_train.data[i])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab98ebf-026b-4096-bfcd-276016a52c6e",
   "metadata": {},
   "source": [
    "**Pre-processing Data**\n",
    "\n",
    "In this section, we focus on preparing the text data from the 20 Newsgroups dataset for our semantic search engine. Preprocessing is a critical step to ensure the quality and consistency of the data before it's fed into the Universal Sentence Encoder.\n",
    "\n",
    "**Steps in Preprocessing:**\n",
    "\n",
    "1. **Fetching Data**: \n",
    "   - We load the complete 20 Newsgroups dataset using `fetch_20newsgroups` from `sklearn.datasets`. \n",
    "   - `documents = newsgroups.data` stores all the newsgroup documents in a list.\n",
    "\n",
    "2. **Defining the Preprocessing Function**:\n",
    "   - The `preprocess_text` function is designed to clean each text document. Here's what it does to every piece of text:\n",
    "     - **Removes Email Headers**: Strips off lines that start with 'From:' as they usually contain metadata like email addresses.\n",
    "     - **Eliminates Email Addresses**: Finds patterns resembling email addresses and removes them.\n",
    "     - **Strips Punctuations and Numbers**: Removes all characters except alphabets, aiding in focusing on textual data.\n",
    "     - **Converts to Lowercase**: Standardizes the text by converting all characters to lowercase, ensuring uniformity.\n",
    "     - **Trims Excess Whitespace**: Cleans up any extra spaces, tabs, or line breaks.\n",
    "\n",
    "3. **Applying Preprocessing**:\n",
    "   - We iterate over each document in the `documents` list and apply our `preprocess_text` function.\n",
    "   - The cleaned documents are stored in `processed_documents`, ready for further processing.\n",
    "\n",
    "By preprocessing the text data in this way, we reduce noise and standardize the text, which is essential for achieving meaningful semantic analysis in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f510ebd-7293-4e6f-b32b-aa9d7acdfc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "documents = newsgroups.data\n",
    "\n",
    "# Basic preprocessing of text data\n",
    "def preprocess_text(text):\n",
    "    # Remove email headers\n",
    "    text = re.sub(r'^From:.*\\n?', '', text, flags=re.MULTILINE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess each document\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435674f8-e53a-458a-996b-af3aa2897977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sample post to display\n",
    "sample_index = 0  # for example, the first post in the dataset\n",
    "\n",
    "# Print the original post\n",
    "print(\"Original post:\\n\")\n",
    "print(newsgroups_train.data[sample_index])\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the preprocessed post\n",
    "print(\"Preprocessed post:\\n\")\n",
    "print(preprocess_text(newsgroups_train.data[sample_index]))\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3548d-04ad-47b4-93ed-80a4512bbb81",
   "metadata": {},
   "source": [
    "**Universal Sentence Encoder**\n",
    "\n",
    "After preprocessing the text data, the next step is to transform this cleaned text into numerical vectors using the Universal Sentence Encoder (USE). These vectors capture the semantic essence of the text.\n",
    "\n",
    "**Loading the USE Module:**\n",
    "\n",
    "- We use TensorFlow Hub (`hub`) to load the pre-trained Universal Sentence Encoder.\n",
    "- `embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")` fetches the USE module, making it ready for vectorization.\n",
    "\n",
    "**Defining the Embedding Function:**\n",
    "\n",
    "- The `embed_text` function is defined to take a piece of text as input and return its vector representation.\n",
    "- Inside the function, `embed(text)` converts the text into a high-dimensional vector, capturing the nuanced semantic meaning.\n",
    "- `.numpy()` is used to convert the result from a TensorFlow tensor to a NumPy array, which is a more versatile format for subsequent operations.\n",
    "\n",
    "**Vectorizing Preprocessed Documents:**\n",
    "\n",
    "- We then apply the `embed_text` function to each document in our preprocessed dataset, `processed_documents`.\n",
    "- `np.vstack([...])` stacks the vectors vertically to create a 2D array, where each row represents a document.\n",
    "- The resulting array `X_use` holds the vectorized representations of all the preprocessed documents, ready to be used for semantic search indexing and querying.\n",
    "\n",
    "By vectorizing the text with USE, we've now converted our textual data into a format that can be efficiently processed by machine learning algorithms, setting the stage for the next step: indexing with FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20485ab7-9d1d-4145-a374-2900e889444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Function to generate embeddings\n",
    "def embed_text(text):\n",
    "    return embed(text).numpy()\n",
    "\n",
    "# Generate embeddings for each preprocessed document\n",
    "X_use = np.vstack([embed_text([doc]) for doc in processed_documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf92eee-6ffc-407c-8ee9-1554b7ad9283",
   "metadata": {},
   "source": [
    "**Indexing with FAISS**\n",
    "\n",
    "With our documents now represented as vectors using the Universal Sentence Encoder, the next step is to use FAISS (Facebook AI Similarity Search) for efficient similarity searching.\n",
    "\n",
    "**Creating a FAISS Index:**\n",
    "\n",
    "- We first determine the dimension of our vectors from `X_use` using `X_use.shape[1]`.\n",
    "- A FAISS index (`index`) is created specifically for L2 distance (Euclidean distance) using `faiss.IndexFlatL2(dimension)`.\n",
    "- We add our document vectors to this index with `index.add(X_use)`. This step effectively creates a searchable space for our document vectors.\n",
    "\n",
    "**Choosing the Right Index:**\n",
    "\n",
    "- In this project, we use `IndexFlatL2` for its simplicity and effectiveness in handling small to medium-sized datasets.\n",
    "- FAISS offers a variety of indexes tailored for different use cases and dataset sizes. Depending on your specific needs and the complexity of your data, you might consider other indexes for more efficient searching.\n",
    "- For larger datasets or more advanced use cases, indexes like `IndexIVFFlat`, `IndexIVFPQ`, and others can provide faster search times and reduced memory usage. Explore more at [FAISS indexes wiki](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81c59f5e-f033-4347-9262-cbb9989b1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = X_use.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # Creating a FAISS index\n",
    "index.add(X_use)  # Adding the document vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38927139-b977-412b-9797-9ce82bee934d",
   "metadata": {},
   "source": [
    "**Quering with FAISS**\n",
    "**Defining the Search Function:**\n",
    "\n",
    "- The `search` function is designed to find documents that are semantically similar to a given query.\n",
    "- It preprocesses the query text using the `preprocess_text` function to ensure consistency.\n",
    "- The query text is then converted to a vector using `embed_text`.\n",
    "- FAISS performs a search for the nearest neighbors (`k`) to this query vector in our index.\n",
    "- It returns the distances and indices of these nearest neighbors.\n",
    "\n",
    "**Executing a Query and Displaying Results:**\n",
    "\n",
    "- We test our search engine with an example query (e.g., \"motorcycle\").\n",
    "- The `search` function returns the indices of the documents in the index that are most similar to the query.\n",
    "- For each result, we display:\n",
    "   - The ranking of the result (based on distance).\n",
    "   - The distance value itself, indicating how close the document is to the query.\n",
    "   - The actual text of the document. We display both the preprocessed and original versions of each document for comparison.\n",
    "\n",
    "This functionality showcases the practical application of semantic search: retrieving information that is contextually relevant to the query, not just based on keyword matching. The displayed results will give a clear idea of how our semantic search engine interprets and responds to natural language queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51620620-d21b-489e-908f-21e2ccbfe4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a query using the Faiss index\n",
    "def search(query_text, k=5):\n",
    "    # Preprocess the query text\n",
    "    preprocessed_query = preprocess_text(query_text)\n",
    "    # Generate the query vector\n",
    "    query_vector = embed_text([preprocessed_query])\n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_vector.astype('float32'), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example Query\n",
    "query_text = \"motorcycle\"\n",
    "distances, indices = search(query_text)\n",
    "\n",
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Ensure that the displayed document is the preprocessed one\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{processed_documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5a55a-b79d-46e4-aeaf-3323235ecdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Displaying the original (unprocessed) document corresponding to the search result\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed8a46-36a4-42f9-a73e-b638fdbe38ec",
   "metadata": {},
   "source": [
    "#### Reading - Hierarchical Navigable Small World (HNSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5a19b-3cf8-4cba-b623-5ca446d96e42",
   "metadata": {},
   "source": [
    "![HNSW Reading](\"readings/HNSW.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b1acf-bd47-42db-8d21-39eb831f76ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (langchain-learn)",
   "language": "python",
   "name": "langchain-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
